[
["index.html", "Decimotercera Edición - ECYS", " Decimotercera Edición - ECYS Escuela de Ingeniería en Ciencias y Sistemas. ECYS 2019-06-15 "],
["00_preface.html", "Editorial", " Editorial 13ª. Edición Revista Digital – Escuela de Ingeniería en Ciencias y Sistemas Actualmente, el concepto de “emprendimiento” está tomando auge y es de gran importancia a nivel mundial, en esencia, busca desarrollar ideas innovadoras que promuevan el empleo y el desarrollo social. Sin embargo, ¿Cómo logramos producir profesionales que puedan desarrollar emprendimientos exitosos en sus áreas de especialización? Esta interrogante es la clave del éxito de instituciones académicas de renombre a nivel mundial. Para lograr emprendimientos exitosos se requiere fomentar la investigación, en el área de tecnología de la información, adicionalmente, debemos enfocarnos en la invención, la innovación y la optimización. Adicionalmente a la investigación, se debe promover una alianza con empresas privadas e instituciones públicas cuyo objeto sea facilitar el acceso a capital de inversión para emprendedores, y buscar desarrollar productos y servicios que estimulen el crecimiento de las empresas existentes en Guatemala, o bien, la creación de nuevos conceptos que generen nuevos mercados donde los guatemaltecos puedan desarrollarse. Esta revista digital, es un paso que la Escuela de Ingeniería en Ciencias y Sistemas ha dado, para fomentar el pensamiento innovador de los estudiantes; se busca que el estudiante encuentre temas y tecnologías que lo apasionen, que complementen su formación y, finalmente, los inspiren para enfocar su esfuerzo en lograr alianzas que lo lleven a la generación de un emprendimiento factible. MA Ing. Marlon Antonio Pérez Türk Director Escuela de Ingeniería en Ciencias y Sistemas Universidad de San Carlos de Guatemala Director General Ing. Marlon Antonio Pérez Türk Coordinación Editorial Ing. Álvaro Giovanni Longo Morales Colaboración Especial Ing. Miguel Marin de León Inga. Gladys Sucely Aceituno Editor Mario Roberto Morales Sitavi "],
["01_mgabriel.html", "Beneficios del software libre en el sistema educativo guatemalteco", " Beneficios del software libre en el sistema educativo guatemalteco Marcelo Gabriel Seisdedos Javier chelo6d2@gmail.com Estudiante de Ingeniería en Ciencias y Sistemas - USAC Palabras clave: Educación, tecnología, software, software propietario, software libre, Edulibre. En un país tan desigual como Guatemala, en el cual no todas las personas tienen acceso a la educación y el sistema educativo actual no cubre todas las necesidades de la población en los diferentes ejes como infraestructura, cobertura, competencias y habilidades, no es algo fuera de lo común descubrir que la sociedad actual no tiene el desarrollo tecnológico esperado en comparación con otros países de América Latina. Según informes proporcionados por el Ministerio de Educación de Guatemala en su Anuario Estadístico 2016, el 81% de la población total recibe su formación primaria, 47% formación básica y el 24% formación de diversificado. Por otro lado, según un informe publicado por la Unicef tratando el tema de la educación en Guatemala, se indicó que el 26% de la población total entre los 7 y los 14 años de edad no asisten a la escuela primaria. Tal vez a simple vista estos porcentajes y cifras no sean tan alarmantes, pero si nos detenemos un momento para pensar el significado de estos resultados, podríamos obtener una radiografía de la situación actual guatemalteca en cuanto a educación. De cada 100 niños, 26 no asisten nunca a la escuela primaria. De la población total, incluyendo niños y adultos, de cada 100 personas, 19 no reciben su formación primaria. Si analizamos los demás datos para la formación básica y diversificado, los datos son aún más alarmantes. Figura 1: Autor: Prensa Libre: José Rosales - http://d3ustg7s7bf7i9.cloudfront.net/mmediafiles/pl/e1/e19631fe-c760-41fb-996d-2b713c3f946f_749_499.jpg Enfocándonos en la fracción de la población que sí recibe su formación primaria, cabe mencionar que aproximadamente el 87% de la población la recibe en un establecimiento público. El pensum educativo del Ministerio de Educación de Guatemala para la educación primaria no tiene contemplado clases obligatorias de computación. Es más, la mayoría de establecimientos públicos no cuentan con un laboratorio de computación. El otro 13% de la población recibe su educación primaria en un establecimiento privado, donde la mayoría de éstos sí tienen contemplado impartir clases de computación desde la primaria. Hablando únicamente de los establecimientos en cuyo pensum de estudios sí tienen contemplado las clases de computación, y considerando que, aunque el porcentaje de la población con acceso a estas competencias en Guatemala es mínimo, es importante inspeccionar de cerca las herramientas que utilizan para obtener estas competencias, ya esta pequeña fracción de la población será en un futuro la población tecnificada que laborará en el mercado guatemalteco. Hablando únicamente de los establecimientos en cuyo pensum de estudios sí tienen contemplado las clases de computación, y considerando que, aunque el porcentaje de la población con acceso a estas competencias en Guatemala es mínimo, es importante inspeccionar de cerca las herramientas que utilizan para alcanzarlas, ya que esta pequeña fracción de la población será en un futuro la población tecnificada que laborará en el mercado guatemalteco. La mayoría de establecimientos privados para impartir sus clases de computación utilizan software propietario. ¿Qué es el software propietario? Es aquel software en el cual un usuario tiene limitadas sus posibilidades de utilizarlo, modificarlo o redistribuirlo, y a menudo su licencia tiene un coste. En Guatemala, el sistema operativo utilizado ‘por defecto’ es Microsoft Windows. Se ha popularizado tanto que es casi seguro que un establecimiento privado lo utilice para sus clases de computación. Esto tiene sus ventajas, claro está, y es un esfuerzo adicional que realizan los establecimientos para adquirirlo, pero la realidad está alejada de esto. Muchos establecimientos privados pequeños adquieren este tipo de software de manera ilegal, sin pagar las licencias correspondientes y utilizando una serie de parches creados sin autorización de los fabricantes para activar los programas. De igual manera, los estudiantes deben realizar sus tareas y proyectos desde su casa utilizando el mismo software propietario que utilizan en sus establecimientos. Muchas de las licencias y programas no son accesibles. Todo esto tiene un costo elevado, y parece que nadie lo ha contemplado. ¿Qué aspecto negativo tiene esto para los estudiantes? ¿Qué se les está enseñando implícitamente? ¿A qué inducen indirectamente los establecimientos a los alumnos? A buscar formas de utilizar software propietario sin pagar. Se les está enseñado y se les induce a los alumnos a cometer actos antiéticos. A utilizar algo sin pagar el precio, a evadir las reglas, los lineamientos, a limitarse. ¿Es en realidad esto lo que se quiere enseñar a los niños? Tomando en cuenta que la Visión del Ministerio de Educación de Guatemala es contribuir al desarrollo integral de los guatemaltecos, con base en principios, valores y convicciones que fundamenten su correcta conducta, la poca o nula regulación del software en los centros educativos es un impedimento para alcanzar las metas del MINEDUC. Figura 2: Autor: Colegio Bilingüe Especializado en Computación Yulimay PC - http://www.colegioyulimaypc.com/images/colegio-01.jpg ¿Cómo podemos remediar esta situación? La solución es bastante simple, y muy fácil de implementar: reemplazar el software propietario por software libre. El software libre es aquel en el que los usuarios tienen cuatro libertades fundamentales sobre él: la libertad de usar el programa con cualquier propósito, la libertad de acceso al código fuente del programa, la libertad de copiado y distribución y la libertad de modificar y mejorar el programa. ¿Por qué software libre y educación deberían ir de la mano? Para empezar, la principal función de la educación es la de brindar el acceso al conocimiento. El software libre nos brinda libertades para poder ejercer esa apropiación del conocimiento, otorgando una gran cantidad de beneficios para la educación: El software libre crea personas libres, independientes y críticas, ya que al utilizar programas con licencia gratuita, esto elimina la dependencia hacia un único tipo de programa, ofreciendo libertad para elegir el programa que más nos agrade. Reduce costos. Nos ahorramos la compra de licencias para sistemas operativos y programas, y además el software está diseñado para ser utilizado por equipos no tan potentes. Facilita a los alumnos a que trabajen en casa con las mismas herramientas que utilizan en la escuela, y todo obtenido de forma legal. Sin embargo, es importante considerar que el porcentaje de la población en Guatemala que tiene en su hogar al menos una computadora aún es muy bajo, siendo los café internet la única forma de tener acceso a una. Permite obtener habilidades que el software propietario no permite, como aprender a utilizar una tecnología, en lugar de un producto. Figura 3: Autor: Coding or Not - https://codingornot.com/wp-content/uploads/2017/05/software-libre.png Además de todos los beneficios que obtenemos al utilizar software libre, cabe mencionar todo el software libre que existe pensados exclusivamente para la educación. Por mencionar algunos, en temas de lenguaje existen programas para agilizar la lectura, como Katamotz. En temas de matemática, existen juegos interactivos como Math Command y Tux Math Scrabble pensados para reforzar el cálculo mental de operaciones matemáticas. El software libre no se limita, y encontramos programas orientados a la química, física, astronomía, geografía y muchos más. El camino para el cambio parece no ser tan viable en este momento, aunque ya existan asociaciones en el país que pretenden ser agentes de cambio, como lo es Edulibre. Su misión es brindar la oportunidad a las niñas y niños de Latino América de tener acceso a una Educación de calidad a través de tecnologías de información, guiados por los principios de código abierto. Actualmente en Guatemala, realizan proyectos de donación de equipo de computación y la instalación de laboratorios de cómputo en escuelas públicas del país. La Escuela de Ingeniería en Ciencias y Sistemas de la Facultad de Ingeniería de la Universidad de San Carlos de Guatemala, con base en su principio de “id y enseñad a todos” participa activamente con la asociación EduLibre para que la idea de implementar el software libre en la educación sea una realidad. Figura 4: Autor: Edulibre.net - http://edulibre.net/wp-content/uploads/2015/06/Logo-Edulibre-website12.png Conclusiones: El sistema de educación actual de Guatemala no tiene contemplado en su pensum de estudios el desarrollo de competencias tecnológicas. La gran mayoría de instituciones que imparten clases de computación utilizan software propietario, siendo el más predominante Microsoft Windows. Es responsabilidad del MINEDUC crear las regulaciones adecuadas para garantizar que en los centros educativos se utilice Software Libre para impartir las clases de computación. El Plan Estratégico de Educación del MINEDUC (2016-2020), en su Eje de espacios dignos y saludables para el aprendizaje tiene contemplado la creación de acceso a la tecnología informática en los distintos niveles educativos. Considerando esto, para futuros Planes Estratégicos se podría regular que las clases de computación en los centros educativos se impartan utilizando Software Libre. De igual forma, crear regulaciones para los centros educativos privados. El software libre y la educación comparten su razón de ser y un propósito, el cual es brindar el acceso al conocimiento a todas las personas. La utilización de software libre brindaría una gran cantidad de beneficios tanto a los estudiantes como a las instituciones educativas, entre las cuales mencionamos el libre y gratuito acceso a los sistemas operativos y programas, el desarrollo de competencias tecnológicas, la libertad de creatividad de los alumnos y la eliminación de la dependencia hacia ciertos productos específicos. El software libre se basa en cuatro libertades principales: libertad de usar el programa con cualquier propósito, de acceso al código fuente del programa, de copiado y distribución y la libertad de modificar y mejorar el programa. Existe gran cantidad software libre educativo, entre los cuales podemos mencionar orientados a la matemática, a la ciencia, física, química, astrología y comunicación y lenguaje. Actualmente en Guatemala, la asociación Edulibre está impulsando la educación de calidad a través de tecnologías de información guiados por los principios de software libre. Referencias: Gobierno de la República de Guatemala, Ministerio de Educación. Anuario Estadístico de la Educación 2016 Guatemala, C.A. http://estadistica.mineduc.gob.gt/anuario/ (14/02/2018) Gobierno de la República de Guatemala, Ministerio de Educación. Sistema Nacional de Indicadores Educativos de Guatemala. http://estadistica.mineduc.gob.gt/ (14/02/2018) Muñoz Palala, Geldi (11/12/2015). Prensa Libre: Educación seguirá deficiente en el 2016, dice ministro. (14/02/2018) Unicef. La Educación en Guatemala. https://www.unicef.org/guatemala/spanish/resources_2562.html (14/02/2018) Nasheli (21/10/2014). Ventajas de utilizar software libre en la educación. (15/02/2018) De Haro, Juan José (20/08/2011). Software libre en educación: Linux. (15/02/2018) GNU Free Software Fundation. Por qué las escuelas deben usar exclusivamente software libre. https://www.gnu.org/philosophy/schools.es.html (15/02/2018) Edulibre. Software y educación. (http://edulibre.net/index.php/about-us/) (15/02/2018) Universia (22/06/2015). Software Libre: 16 programas pensados para la eduación. "],
["02_kjuventino.html", "Bitcoin y la minería", " Bitcoin y la minería Kevin Cristopher Juventino Alvarado Maldonado Kc.alva33@gmail.com Estudiante de Ingeniería en Ciencias y Sistemas - USAC Palabras clave: Bitcoin, moneda, transacciones virtuales, costo incremental, minería, blockchain El bitcoin es de código abierto y los movimientos efectuados en ella son completamente anónimos. Para operar con ellos es necesario almacenarlos en un monedero virtual el cual es un servicio que añade una clave criptográfica a las monedas en cada transacción. Esta moneda se utiliza como un medio de intercambio para el pago de bienes y servicios, el cual cuenta con ventajas como que es global, tiene un límite el cual llegara a 21 millones, lo que busca mitigar el problema de la devaluación de la moneda, las transacciones se hacen en tiempo real, es imposible de falsificar además del anonimato en las transacciones, esta última también puede ser una de sus mayores desventajas, porque junto con que su valor sufre de grandes fluctuaciones y no hay garantías que se convierta en una moneda aceptada por todos. Tomando todo esto en cuenta nos podemos imaginar las posibilidades que esta moneda nos brinda, pero existe algo que no todos conocen, la minería de criptodivisas. Autor:JJ Jimenez (15/01/2014) Dirección electrónica de la imagen: https://3cero.com/ventajas-desventajas-bitcoin/ La minera de criptodivisas es el proceso por el cual se comprueba que una transacción no ha sido realizada con anterioridad, y si este el caso, escribir los datos de esa transacción en un libro de registros público de forma ordenada, al cual se le conoce como blockchain, el cual es una cadena de bloques en el cual cada transacción conforma un bloque, y al unirse al resto forman una cadena, se considera que el blockchain es el sistema de comprobación del futuro para cualquier sistema empresarial o mercadotécnico. Aquí es donde entran los mineros, ya que por cada cierto número de anotaciones, se llevan como recompensa un bitcoin o cualquier tipo de moneda que hayan minado. Y como esto parece una manera sencilla de ganar dinero, muchas personas de manera individual se lanzan al minado de criptomoneda ya que es un sistema abierto y se puede acceder sencillamente, pero en realidad para poder hacer cada una de las inserciones en la cadena de bloques se debe realizar miles de comprobaciones dado que existe una cantidad significante de mineros tratando de hacer lo mismo al mismo tiempo y para poder realizar esto se necesita una gran cantidad de recursos, y el más importante es poseer tarjeta gráfica en gran cantidad o de alta calidad y resto de hardware con un rendimiento óptimo porque se necesita que esté opere las 24 horas, tomando en cuenta que esto genera un gran consumo energético. Y en uno de sus efectos más inmediatos y el cual afecta a la comunidad de usuario que utiliza computadores, es el incremento significante en el precio de las tarjetas de video, ya que generalmente por este medio es que se procesan las transacciones mencionadas anteriormente. También se puede mencionar que los mineros han llegado hasta el punto de utilizar recursos de nuestros dispositivos por medio de software malintencionados con tal de seguir ganando más monedas. Autor:MAR BARBERÁ Dirección electrónica de la imagen: http://socialanimals.buzzmn.com/wp-content/uploads/2013/02/bitcoin.jpg Conclusiones: El bitcoin nos abre muchas posibilidades en el marcado económico, desde ejemplos de cómo en sus inicios un individuo realizo la compra de una pizza por 2000 bitcoins, el cual en ese momento le pareció un buen trato, pero esa misma cantidad de bitcoin en el presente tiene un valor de varios cientos de miles de dólares, el cual es una simple muestra del incremento exagerado de valor que ha tenido esta moneda, y de como nosotros podemos generar ganancias con su minería, todo esto tiene sus ventajas y desventajas como se ha mencionado, lo importante es que haremos con esta información. Referencias: Bitcoin Que es y para que se utiliza. Mineria de criptodivisas. http://elreydelblog.es/articulo.php?id=efectos-mineria- bitcoin / (15/8/2017) "],
["03_ggiron.html", "Deep Learning y su increíble impacto en la realidad conocida", " Deep Learning y su increíble impacto en la realidad conocida Gary Stephen Girón Molina gsteph393@gmail.com Estudiante de Ingeniería en Ciencias y Sistemas - USAC Palabras clave: Deep Learning, inteligencia artificial, sociedad, futuro, redes neuronales, realidad aumentada Si bien hoy en día las necesidades de las personas no se ven colapsadas por algún fallo en los sistemas actuales para suplirlas, el constante cambio y aumento de información siguen concentrando y cargando a grados extremos las diferentes herramientas que se utilizan, una cantidad de datos que sería imposible poder analizar sin lo que hoy se nos ha provisto, Big Data, Analytics, entre otros. El surgimiento de lo que hoy conocemos como inteligencia artificial fue la misma causa que nos ha llevado hasta este punto, se ha requerido proporcionar una toma de decisiones y un análisis de información de forma precisa y optima, esto con el fin de evitar toda complejidad que exista dentro de la administración de recursos en todo proceso u operación, por lo cual es necesario tener un concepto amplio de la base que ha llevado a técnicas como Deep Learning a su existencia. Inteligencia Artificial Como definición formal es una rama de las ciencias de computación que se dedican a resolver problemas que no son algorítmicos, mediante las herramientas que se disponen en la misma ciencia, sin tomar en consideración la forma del razonamiento subyacente a todas aquellas técnicas que sean aceptables para lograr dicha resolución, si desglosamos las palabras tenemos que inteligencia es aquella capacidad de entender, analizar, y crear información para luego utilizarla de forma adecuada, por lo cual podemos decir que inteligencia artificial es la capacidad de las máquinas y demás dispositivos de aprender, con la cual tenemos tres técnicas principales, las cuales son: Programación Heurística: Esta basada en el modelo del comportamiento del ser humano, en conjunto con su respectiva forma de buscar problemas. Redes Neuronales: Es una representación abstracta de cómo funciona el modelo cerebral del ser humano. Evolución Artificial: Es una técnica de control del software sobre sí mismo. Redes Neuronales Su concepto las identifica como redes interconectadas de forma paralela, por medio de las unidades mínimas, que tienen las características de simplicidad, adaptabilidad y su respectiva jerarquía. Una red neuronal consta de una capa de entrada, una cantidad determinada de capas ocultas y una capa de salida en la cual devuelve la solución esperada como se puede apreciar en la Imagen 1. Autor: blogdiario Dirección electrónica de la imagen: Enlace El uso de las redes neuronales tiene muchas ventajas, entre ellas tenemos: Lenguaje Adaptativo: Es la habilidad que se posee de realizar las actividades a través de la misma experiencia o entrenamiento. Autoorganización: Es capaz de realizar su propia organización o modelo de la información que recibe en la etapa de aprendizaje. Tolerancia a fallo: algunas capacidades a pesar de sufrir un gran daño se pueden retener en la red. Operación en tiempo real: La utilización de paralelismo Fácil inserción dentro de la tecnología existente: Se pueden usar chips para aumentar su capacidad y rendimiento. La estructura de las redes neuronales es compuesta por unidades básicas mínimas conocidas como neuronas, las cuales se pueden describir fácilmente como las neuronas biológicas de nuestro cuerpo. Deep Learning Esta técnica se basa en un conjunto de algoritmos de tipo aprendizaje automático (como su predecesor Machine Learning), los cuales intentan modelar las diferentes abstracciones de alto nivel mediante el uso de arquitecturas de transformaciones no lineales, se diferencia de una simple red neuronal debido a la cantidad de capas ocultas que posee como se observa en la Imagen 2. Autor: Ackstorm managed Cloud Solutions Dirección electrónica de la imagen: Enlace Una de las características importantes y que tienen mayor relevancia en el cambio al cual se afronta la sociedad es el enfoque de Deep Learning donde la única intervención humana en su proceso es la programación, luego de ello, esta aprende de sí misma para solventar sus errores, caso contrario del Machine Lerning que actualmente hoy vemos, ya que aunque recibe una entrada, lo clasifica por si misma para dar una salida, una solución, es necesario que una persona realice la extracción de características y vaya guiando el procedimiento, mientras esta guía o trabajo lo reduciría Deep Learning para trabajar y aprender por sí misma como muestra la Imagen 3. Autor: The Startup Dirección electrónica de la imagen: Enlace Para obtener una red funcional en nuestro Aprendizaje Profundo o Deep Learning, debemos considerar cuantas capas vamos a incluir y cuantas neuronas estarán disponibles en cada capa, esto se realiza de forma arbitraria, tomando en cuenta que entre más capas o neuronas por capa se usen, mayor será la complejidad de problemas que lograra resolver, pero de la misma manera, de forma proporcional es más complicado de manejar o realizar, luego de ello se considera el peso concreto que se usara entre cada par de conexiones. Luego de comprender los conceptos generales y algunas especificaciones podemos considerar el punto inicial, ¿cómo afecta al mundo en que vivimos?, la realidad como la conocemos, ¿en qué resiente el cambio y dependencia de estas tecnologías? Impacto de Deep Learning en la sociedad Deep Learning supone un cambio en gran parte de la realidad que conocemos, uno de los ejemplos claros en los que actualmente se utiliza este tipo de técnicas, es en el reconocimiento de imágenes, donde la misma se brinda como entrada y cada pixel es enviado a una cantidad arbitraria de neuronas en las capas ocultas para su procesamiento y análisis, así al finalizar tener nuestro resultado según las especificaciones para lo que se diseñó la herramienta y los resultados no algorítmicos de las operaciones que se generaron entre los pesos ponderados de las conexiones entre las neuronas recorridas hasta llegar al resultado, un ejemplo claro es la forma en que Facebook logra identificar los rostros en las fotografías que se suben a la red, así como asociarlo a un perfil al cual se podría etiquetar. La idea del reconocimiento facial adjudicado a nuevas tecnologías como el Internet de las cosas, donde cada dispositivo, cada cámara u objeto en el cual sea posible recabar información está conectado a la red y el respectivo análisis y almacenamiento de Big Data en la nube complementarían herramientas de reconocimiento a nivel mundial, en la cual se ubique todo sobre la faz de la tierra en cuestión de segundos, en el peor de los escenarios minutos con solo estar a la vista de algún dispositivo. Deep Learning revolucionaria otros campos de la misma manera como la forma en que nos vemos inmersos en nuestra vida. ¿Cuántas veces hemos estado en un tráfico interminable? ¿Cuál sería el cambio si el mismo vehículo pudiera conducirse y tomar sus propias decisiones para llevarnos a nuestro destino deseado? El tiempo se aprovecharía de mayor manera, la frustración que conlleva las horas y horas perdidas en el tráfico se solucionarían, pero a la vez gradualmente estaríamos dependiendo más de los artefactos de nuestro entorno, actualmente personajes reconocidos como Steve Wosniak en entrevistas recientes han declarado que tienen fe en tecnologías emergentes como las que se mencionaban anteriormente, el reconocimiento de voz y otras, actualmente nos encontramos en una transición que nos lleva a un mundo del cual solo tenemos la esencia inicial, uno que al momento de fallar la tecnología en la cual está basado podría significar un desastre debido a que las condiciones de vida no serían las conocidas por cada individuo. Conclusiones: La realidad que conocemos está definida por los ejes en los cuales se basa nuestra forma de vivir. Deep Learning sugiere algún problema a gran escala en la sociedad, debido a la renuencia al impacto que generarían dichas tecnologías. Conforme las necesidades del ser humano aumentan, es necesario tener un cambio que logre suplir dichas necesidades. Actualmente nos encontramos en una transición que nos lleva a una sociedad desarrollada que logre optimizar factores como el tiempo. Entre más nos trasladamos a un mundo virtualizado, más dependemos de dicha tecnología. Un fallo de las bases de la sociedad provocaría un desastre debido a la falta de conocimiento de la forma de vida que esto implicaría. Referencias: 1. TechTarget (30/4/2017). Aprendizaje profundo (deep learning). (15/2/2018) 2. Rubén López (7/5/2014). ¿Qué es y cómo funciona “Deep Learning”?. (12/2/2018) "],
["04_jruiz.html", "Hablemos de licencias de software", " Hablemos de licencias de software Juan Pablo Ruiz Guerra jruizg96.11@gmail.com Estudiante de Ingeniería en Ciencias y Sistemas - USAC Palabras clave: Licencia, software, open source, software, privativo. Para poder analizar correctamente los tipos de licencia, primero es necesario responder… ¿Qué es una la licencia de software? En términos muy simples, una licencia de software es un contrato entre la entidad propietaria del software y los consumidores, en el cual se establecen los derechos y obligaciones de ambas partes. Habiendo definido la licencia de software se podrá hablar más a detalle de las características de cada una en específico. La primera que se analizará es el tipo de licencia de código cerrado o software privativo debido a que es el más común. Su ideología se basa en proteger los derechos de autor y propiedad intelectual del fabricante. Este tipo de licencia establece las restricciones de uso, distribución, modificación y otros factores sobre el software, en la mayoría de casos prohibiendo los mismos. En casi todos los casos se debe pagar para tener el software. Por supuesto que se percibe un poco egoísta e interesado, cuantas personas están dispuestas a regalar su trabajo o permitir que utilicen sus productos para sus beneficios sin costo alguno. Unos ejemplos de este tipo serían los que normalmente se aceptan al utilizar los programas de Microsoft o cuando las empresas obtienen productos SAP. Las principales ventajas que ofrecen estos programas es la seguridad de que se ha invertido una gran cantidad de tiempo y recursos en desarrollar un software de calidad y que el soporte técnico para la herramienta será bastante bueno. Autor: Contenidosdigitales/desconocido Dirección electrónica de la imagen:http://contenidosdigitales.ulp.edu.ar/exe/sistemadeinfo/image001.png Por el contrario, el que más tiende a agradar en el área de informática debido a que da el control sobre el software. El tipo de licenciamiento open source o código abierto ofrece el código fuente de los programas que se están adquiriendo. Normalmente, se piensa que el adquirir software de este tipo es gratis y totalmente permisivo, pero esto no es así. Si bien es cierto que este tipo de licenciamiento permite acceder al código fuente, modificarlo y distribuirlo, esto no quiere decir que no pueda cobrar por ello y que no pueda poner ciertas restricciones. Es por esto que este tipo de licenciamiento se puede clasificar de la forma siguiente: Licencias de código abierto permisivo: no pone restricciones sobre su uso, modificación o distribución. Un ejemplo de este tipo seria la licencia “Academic Free License”. Licencias de código abierto con restricciones débiles: contienen una cláusula que obliga que las modificaciones que se realicen al software original se deban licenciar bajo los mismos términos y condiciones. Sin embargo, las copias derivadas de él sí pueden ser licenciados bajo otros términos y condiciones. Un ejemplo de este tipo seria la licencia \"Mozilla Public License\". Licencias de código abierto con restricciones fuertes: contienen una cláusula que obliga que las modificaciones y obras derivadas que se realicen del software original se deban licenciar bajo los mismos términos y condiciones. Un ejemplo de este tipo seria la licencia “GNU General Public License”. Las ventajas de las licencias de código abierto son que normalmente son gratuitas o de muy bajo costo, permite adaptar las herramientas a las necesidades del cliente y se pueden conseguir con mucha facilidad. Las desventajas son primordialmente el soporte, en este tipo de licenciamiento el soporte es dado por la comunidad, si hay una comunidad pequeña usando el software habrá poco soporte. Por último, tenemos el tipo de licencia de dominio público o software sin licencia. Este tipo de software no pone restricción alguna. Eso quiere decir que permite su uso, copia, modificación y distribución con o sin fines de lucro. Por supuesto que cada tipo de licenciamiento tiene diferentes variaciones, pero si se analiza un poco desde diferentes puntos de vista, para los usuarios es conveniente tener una herramienta que tenga soporte, actualizaciones, parches de seguridad, etc. Todo esto lo ofrece el tipo de licenciamiento de código cerrado, pero si existe un software de código abierto que cumpla con los requisitos y que tenga una comunidad lo suficientemente grande para brindar el soporte puede que sea más conveniente esta opción. Para un desarrollador, existen herramientas de código abierto que, al modificarlo, pueden reducir el tiempo y el costo de hacer determinado producto, pero puede ser que si lo usemos lo tengamos que liberar bajo los mismos términos de licenciamiento, por lo que puede que esto no sea conveniente. La mejor opción siempre dependerá del contexto en el que se encuentre. Lo mejor en todo momento será informarse de todas las maneras posibles, y decidir con base en modas o favoritismos, y claro leer los términos y condiciones que siempre se ignoran. Conclusiones: La elección de un tipo de licenciamiento es fundamental al momento de empezar un proyecto, es necesario considerar todos los factores que pueden afectar de alguna manera al proyecto. El soporte que pueda tener cada licencia que se incluya en un proyecto puede llegar a impactar directamente en el éxito o fracaso del proyecto. La licencia de código abierto no necesariamente debe ser gratuita. La licencia de código abierto puede imponer restricciones en tus modificaciones y obras derivadas. La mejor elección en el tipo de licencia siempre dependerá del contexto en que se encuentre. Referencias: ¿Conoces los tipos de licencia de software? Aquí te desglosamos el tema. Ing. Victor Bazán Sanchez. Licencias de software (27/01/2018). Tipos de licencia de software. "],
["05_alopez.html", "Cómo elegir el procesador adecuado en base a sus características y nuestras necesidades", " Cómo elegir el procesador adecuado en base a sus características y nuestras necesidades Abner Lorenzo López Chávez abner.lopezchavez@gmail.com Estudiante de Ingeniería en Ciencias y Sistemas - USAC Palabras clave: Intel, procesador, generaciones de procesadores, diferencias entre procesadores, características del procesador. Cada vez que actualizamos algún equipo como: computadora, Tablet, teléfono inteligente, reloj inteligente; es porque nuestro equipo anterior quedó obsoleto, necesitamos uno más potente o por desperfecto del actual. Una de las partes más importantes de todos los dispositivos mencionados anteriormente es el procesador, por lo que al actualizar nuestro dispositivo debemos asegurarnos que estamos escogiendo el adecuado. Como existe una diversidad de procesadores, es necesario conocer un poco más a fondo sus características, cómo diferenciar entre uno y otro, cuál se acopla a nuestras necesidades, sobres los procesadores para elegir el correcto. Existen muchas limitantes, como nuestro presupuesto, disponibilidad del producto, por mencionar algunas; por lo que nuestra elección también dependerá de todos estos factores buscando el de mayor costo-beneficio. MATERIALES Y MÉTODOS Obtención de la información necesaria La información que proveen muchos comerciantes de equipos electrónicos ha mejorado con el paso del tiempo. En un inicio solamente se mencionaban características muy esenciales como: cantidad de memoria, versión general del procesador, tamaño de pantalla; pero al adquirir un nuevo equipo percibimos que, aunque fuere un equipo reciente, no satisface nuestras expectativas. Ahora los comerciantes saben que los usuarios son más exigentes al momento de adquirir un equipo nuevo, se realizan preguntas como ¿qué velocidad tiene la memoria RAM? ¿a qué velocidad transmite información el disco duro? ¿cuál es la velocidad del procesador? Si está a nuestro alcance acceder a los equipos de muestra, existen diferentes formas para obtener la información del hardware de dicho equipo, según el sistema operativo del equipo. Caso contrario, necesitamos obtener el modelo del producto: Toshiba Satellite, Dell Inspiron, como ejemplos; además de esto, la línea del producto: A5310, i77577 respectivamente, esto se debe a que en un mismo modelo existen diferentes líneas de productos con diferencias en el hardware. Con esta información obtendremos datos concretos, como: cantidad de memoria RAM; un disco duro de tal capacidad; procesador Intel Core i3 3120M, el dato que nos interesa; pero ¿qué significa esto? Lo primero que nos viene a la mente es escribir esta información en algún buscador web y encontraremos la información oficial en el sitio web del fabricante de procesadores. Procesando la información obtenida Una vez que tenemos información oficial del procesador del equipo en el que tuvimos interés: el número de núcleos que dispone el procesador, la cantidad de memoria caché, la velocidad de trabajo y el socket o ranura donde se instala el procesador, en caso de que estemos armando una computadora de escritorio, ya que para un equipo portátil el procesador ya viene instalado en la ranura adecuada. A continuación, describiremos algunas características más relevantes del procesador. Núcleo: es la cantidad de procesadores que conforman el procesador; por ejemplo, si posee ocho ‘cores’ o núcleos, el procesador está compuesto de ocho procesadores que realizan tareas en paralelo para mejorar le efectividad del procesamiento. Aquí podemos pensar que a mayor cantidad de núcleos mejor es el procesador, pero no es del todo cierto. Ilustración 1. Procesador de menor cantidad de núcleos con una velocidad efectiva del 3% versus uno procesador de mayor cantidad de núcleos. Fuente: http://cpu.userbenchmark.com Ilustración 2. Procesador de mayor cantidad de núcleos con una velocidad efectiva del 6% versus un procesador de menor cantidad de núcleos. Fuente: http://cpu.userbenchmark.com Thread o Hilo: A grandes rasgos el hilo es un componente del procesador que ayuda con la administración de las tareas del CPU; mejorando el uso del tiempo de espera entre los procesos en espera o ejecución del procesador. Memoria caché: Es una memoria de velocidad muy alta ubicada en el procesador. Tiene la desventaja de ser muy pequeña, por lo que necesitamos de otros componentes que puedan almacenar mayores volúmenes de información, pero son más lentas, como la memoria RAM. Entre mayor sea la memoria caché del procesador, mayor será la cantidad de información que puede ser procesada. Se subdividen en diferentes niveles como L1, L2, L3. La memoria L1 se caracteriza por estar en dentro del núcleo del procesador volviéndola tan rápida como la velocidad misma del procesador, pero por lo mismo es de un tamaño muy reducido. Las siguientes son de mayor tamaño, pero tienen una velocidad promedio entre el procesador y la memoria RAM, o bien la misma velocidad de la memoria RAM. Velocidad: Fue una de las principales características que se tomaban en cuenta para determinar si un procesador era mejor que otro, pero con la inclusión de núcleos y memoria caché ya no es el único factor que determina la ventaja de un procesador sobre otro. La unidad de medida en la actualidad son los Giga Hertz o abreviado como GHz. Como en los sistemas de medida un Kilo equivale a 1000 unidades, mil Kilos equivalen a un Mega, mil Megas a un Giga, en este caso las unidades son los Hertz; por lo que un Giga Hertz es equivalente a mil millones de Hertz por segundo. Los hercios o Hertz son ciclos o instrucciones por segundo que realiza el procesador. Como vimos en las imágenes anteriores, podemos observar que, tener mayor velocidad de procesamiento no precisamente el procesador sea mejor. En algún caso tener mayor cantidad de núcleos y memoria caché repercute en tener mejor rendimiento. Socket: Es la ranura dónde se instala el procesador. Es el medio que permite la comunicación del procesador con el resto de los componentes. Para equipos portátiles el hardware es diseñado para ese procesador. Es difícil actualizar este componente para equipos portátiles, caso contrario para equipos de escritorio, tienen la ventaja de poder actualizar a un procesador más potente. Para cada modelo existe un socket específico, en algunos casos el procesador será compatible físicamente para diferentes sockets, pero no será reconocido por los demás componentes. La mayoría de los fabricantes especifica los sockets compatibles para cada modelo de procesador. Litografía: El tamaño de los transistores con que está fabricado el procesador. Su unidad de medida es el nanómetro, una milmillonésima parte de un metro; para tener una idea, si una naranja tuviera un diámetro de un nanómetro, la tierra tuviera un diámetro de un metro. Es la unidad mínima fundamental que compone un procesador. Se utilizan para simular compuertas lógicas, ya que permite los estados fundamentales: 0 y 1; combinando mas transistores podemos realizar operaciones más complejas, como operaciones matemáticas, instrucciones a bajo nivel. Modelo y línea: el modelo y la línea del procesador es uno de los datos más relevantes. Buscando en la página oficial del fabricante obtendremos todas las características mencionadas anteriormente; para ejemplificar el procesador Core i3 3120M, la información oficial brindada es la siguiente: Ilustración 3. Información brindada por el fabricante, Intel. Fuente: https://ark.intel.com/es/ Con esta fuente oficial podemos saber cuáles son los parámetros del procesador que hemos elegido y nos ayudará a tomar una decisión de lo que este puede realizar. El modelo del procesador como referencial Hasta este punto tenemos mucha información: modelo del procesador, su velocidad, cantidad de núcleos, su memoria interna, algunos casos donde a mayor cantidad de características es mejor, otros dónde no, entre otros. Pero al momento de adquirir un equipo nuevo ¿recordaremos toda esta información? ¿cómo sabemos en qué casos estas características serán mejores que las otras? ¿qué modelo de procesador nos conviene más? Como observamos con el ejemplo: Intel Core i3 3120M; es un código identificador único del modelo del procesador, pero tiene otro propósito: distinguir la línea y generación, y sus características para un objetivo específico. Línea y Generación Algunas de las líneas mencionadas anteriormente fueron: Intel Core, Intel Xeon, Intel Atom, Intel Pentium, Intel Celeron, Intel Itanium e Intel Quark. La información brindada por el fabricante para cada uno de los anteriores: Intel Core: enfocada hacia “videojuegos intensos, creación de contenido, entretenimiento 4k UHD”, según Intel. Intel Xeon: enfocada hacia “computación en la nube, análisis en tiempo real, procesamiento para negocios y perspectivas en macrodatos. Mejorar la eficacia y confiabilidad del centro de datos para manejar cualquier carga de trabajo”, según Intel. Intel Atom: enfocado hacia “dispositivos móviles, funcionando más rápido y durante mayor tiempo, con soporte de archivos multimedia Ultra HD 4K… con un diseño ultradelgado y ligero”, según Intel. Intel Quark: “permiten el uso de aplicaciones de punta para Internet de las cosas (IoT). Proporcionan conectividad, integración y compatibilidad en un paquete flexible y de bajo consumo de energía”, según Intel. De una generación a otra existen mejoras en: velocidad del reloj, cantidad de núcleos, memoria caché, número de hilos. Para distinguir este dato es muy sencillo, Intel agrega, después del modelo, un número que identifica a la generación, de esta manera: Modelo X123; dónde la X es el número de la generación. Continuando con nuestro ejemplo, el Core i3 3120M; la X es igual a 3, por lo que determinamos que es un procesador de 3ra Generación. La siguiente imagen muestra la comparativa de un procesador de 8va generación contra uno de 7ma generación, con el mismo modelo de diseño; haciendo una analogía, la comparativa entre el modelo de un carro de cierto modelo contra un modelo más reciente. Ilustración 5. Comparativa de un procesador de 8va generación versus su antecesor de 7ma generación. Fuente: http://cpu.userbenchmark.com Para este modelo en específico aumentaron la cantidad de núcleos e hilos en dos respecto del modelo anterior, aunque su velocidad sea menor, tiene una eficiencia de velocidad mayor al 25% respecto de su antecesor. Objetivo específico El último dato brindado por el modelo detallado de un procesador es la línea del producto. La siguiente imagen ayudará a identificar la misma: Ilustración 6. Línea del producto de un procesador Intel. Fuente: https://www.intel.la/content/www/xl/es/processors/processor-numbers.html Con este dato podremos identificar el objetivo para el que fue diseñado el procesador por Intel. Aquí algunos ejemplos: X: Intel Core Serie X, la serie mas potente de la familia Core; para edición de videos y videojuegos. MX: De la familia Intel Core Serie X, para equipos móviles. MQ: procesadores de cuatro núcleos (Quad), para equipos móviles. U: bajo consumo de energía (Ultra-low power). HQ: alto nivel gráfico de cuatro núcleos (High performance graphics, Quad core). Con la gran variedad de modelos, específicos para cada uno: enfocado a equipos móviles o laptops, para videojuegos, etc. La información más detallada la encuentras en la web del fabricante Intel: https://www.intel.la/content/www/xl/es/products/processors.html Procesado toda esta información de manera eficiente Con la breve introducción a los procesadores, sus líneas y modelos, generaciones y demás características; tenemos una infinidad de detalles a tomar en cuenta antes de decidir por algún procesador en específico. Pero ¿realmente existe una forma de elegir el procesador adecuado para nuestra necesidad sin tener que comparar tantas características? No existe un método que nos dé una garantía que elegimos el procesador número uno para lo que necesitamos, ya que como mencioné, dependemos del presupuesto con que contamos, si el procesador que queremos está disponible en los distribuidores locales, etc. Pero los siguientes consejos serán útiles al momento en que decidas elegir un procesador, que en base a sus características tendrá un funcionamiento óptimo para las tareas a las que tenemos pensado someterlo, y que en base a su rendimiento estamos eligiendo un buen procesador en relación calidad precio. Identificar el modelo y línea del procesador: con esto garantizamos que seleccionamos un procesador adecuado para lo que necesitamos. Con los detalles mencionados anteriormente podremos identificar si el procesador está diseñado para: procesamiento para diseño gráfico, óptimo para videojuegos, si es un procesador para equipos móviles o de escritorio. Revisiones del producto: una vez elegido un modelo concreto del procesador, existen diversidad de videos en el sitio web Youtube, de personas dedicadas a hacer revisiones de procesador. Brindan información respecto a su rendimiento, configuraciones óptimas para dicho procesador: memoria RAM, disipador de calor, disco duro, tarjeta madre, tarjeta de video; para tener una idea de cuánto será el presupuesto total al armar el equipo completo, en caso de ser un equipo de escritorio, para equipos portátiles nos dirán fabricantes y modelos de portátiles que traen incluidos dicho procesador y en qué configuraciones podremos encontrarlas. Pruebas en tiempo real: De igual forma, existen otros usuarios que realizan pruebas usando programas de ‘benchmarking’ para mostrar información en tiempo real, bajo ciertas situaciones específicas de cada procesador: corriendo videojuegos, renderizando imágenes, ejecutando tareas para utilizar el procesador al máximo, el calor producido por el procesador en estas situaciones, etcétera. Muchos de estos escenarios son los mismos a los que será sometido constantemente por parte del usuario interesado, por lo que nos darán una idea de su rendimiento para nuestra propias necesidades. Sitios web de benchmarking: como en las imágenes anteriores, un sitio de benchmarking es muy buena referencia para identificar un buen procesador. Puedes realizar comparativas, dependiendo del sitio, de dos o varios procesadores. El sitio web automáticamente te dirá cuál es, según sus datos, el mejor procesador de los comparados con base a criterios como: velocidad efectiva, comparativo promedio por parte de los usuarios, relación calidad precio, y una conclusión en general. Contienen ranking general de una gran mayoría de procesadores existentes, en base a todos los criterios anteriores. LITERATURA CITADA: https://ark.intel.com https://intel.la https://cpu.userbenchmark.com Carvajal Jr, Hector. Diferencias entre generaciones de procesadores Intel. Espeso, Pablo. La importancia de los nanómetros en los procesadores. Zúñiga, César. Principales características de un Procesador. "],
["06_ymarroquin.html", "Simulación como tecnología moderna para la salud en Centroamérica", " Simulación como tecnología moderna para la salud en Centroamérica Yessenia Janira Marroquín Martínez Ymjm93@gmail.com Estudiante de Ingeniería en Ciencias y Sistemas - USAC Palabras clave: Simulación, modelación, software, simio, tecnología. Las simulaciones médicas, en general, pretenden imitar pacientes reales, regiones anatómicas o tareas clínicas, y / o reflejar las circunstancias de la vida real en que se prestan los servicios médicos. El término simulación en este artículo se refiere a dispositivos de simulación particulares. Estos pueden tomar muchas formas y abarcar el rango de baja a alta fidelidad, y desde dispositivos para usuarios individuales hasta simulaciones para grupos de aprendices. Autor: Desconocido Dirección electrónica de la imagen: https://danielrparente.files.wordpress.com/2013/01/human3d.jpg Simulación en Centroamérica La simulación es relativamente nueva en muchos países de bajos ingresos, generalmente es en los hospitales prestigiosos donde se ve la aplicación de los simuladores. La palabra \"simulación\" implica una imitación de un proceso de la vida real para proporcionar una experiencia realista en un entorno controlado. Se puede considerar como un lugar para aprender de los errores sin causar ningún daño, por ello en los hospitales utilizan la simulación para entrenar y evaluar el trabajo en equipo al momento de una operación o rutinas diarias. Los eventos de simulación muestran una retroalimentación de cómo se reaccionaría en situaciones de la vida real y, en cierto sentido, muestran cómo funcionan los procesos inconscientes. Ya que no está ocurriendo en la vida real, la simulación permite aprender de los errores. Como tal, puede ayudar a prevenir errores y optimizar respuestas en situaciones críticas. Por ejemplo, el ruido u otras perturbaciones pueden simularse y dar una buena idea de cómo funciona la distracción, otro ejemplo puede ser la aglomeración de pacientes para un diagnostico y prever cuantas máquinas se necesitan para cumplir con la demanda de pacientes. Costa Rica cuenta con un hospital de simulación en la Universidad Hispanoamericana, donde brindan un espacio de práctica donde se emplee la simulación clínica como metodología educativa, para que los estudiantes puedan desarrollar las competencias técnicas y sociales propias de las carreras de Ciencias de la Salud, manteniendo la seguridad de la persona como pilar.  Al aprender en un entorno libre de riesgos, se pueden cometer errores sin daño potencial para los pacientes, y las conductas pueden ajustarse de acuerdo con el desarrollo del dominio de habilidades y habilidades. La capacitación en simulación brinda una oportunidad invaluable para que los estudiantes de atención médica apliquen la teoría para practicar y entrenar en procedimientos y técnicas a los que de otra manera no tendrían acceso, todo ello sin correr el riesgo de la seguridad del paciente (es decir, durante situaciones de emergencia y reanimación). Asimismo, Costa Rica es el primer país por primera vez en toda Latinoamérica, que cuenta con un centro altamente especializado móvil, este será capaz de trasladarse a zonas rurales y urbanas con el propósito de aumentar la excelencia de los procedimientos clínicos que realizan profesionales de salud a nivel nacional. Se trata del Centro de Simulación Móvil-UCR, a cargo de la Escuela de Enfermería de la Universidad de Costa Rica (UCR), el cual permitirá que tanto estudiantes como enfermeros ya titulados, así como otros expertos dedicados a la atención de pacientes, practiquen diversos tipos de procedimientos clínicos de importante complejidad en modelos humanos antes de intervenir a una persona real. Autor: Laura Rodríguez Rodríguez Dirección electrónica de la imagen: Enlace En caso de simuladores de realidad virtual y aplicación de inteligencia artificial comienza a utilizarse y se convierte en un asistente avanzado en hospitales, por ejemplo, con rayos X, tomografía computarizada (TC), imágenes de resonancia magnética (IRM) o una prueba de ultrasonido, el algoritmo del sistema recrea la imagen en forma tridimensional (3D). Autor: Oscar Vargas Dirección electrónica de la imagen: Enlace Conclusión: La única forma de ver qué simulador modela mejor un escenario o es más preciso es haciendo comparaciones. La pregunta de qué simulador es el mejor en una situación específica es importante para los desarrolladores. Las simulaciones médicas encuentran cada vez más un lugar entre las herramientas de simulación para la enseñanza y la evaluación en salud. Los avances tecnológicos han creado una amplia gama de simuladores que pueden facilitar el aprendizaje y la evaluación en numerosas áreas de la educación médica. La tecnología de simulación es una gran promesa para mejorar la capacitación de los médicos y, por lo tanto, para impactar en la seguridad del paciente y los resultados de la atención médica de una manera positiva y significativa. Referencias: Hospital de Simulación UH. https://uh.ac.cr/desarollocompetencias/detalle/simulacion Centro de Simulación Móvil UCR, único en Latinoamérica, mejorará la seguridad clínica de pacientes. Jenniffer Jiménez Córdoba. Febrero 2018. Artificial Intelligence Already Provides Medical Assistance in Costa Rica. Charito Villegas. Noviembre 2017. "],
["07_dlopez.html", "La nanotecnología en la medicina", " La nanotecnología en la medicina Daniel Armando López Santos Danielss360@gmail.com Estudiante de Ingeniería en Ciencias y Sistemas - USAC Palabras clave: Tecnología, nanotecnología, medicina, nanotubos, nanoestructuras, nanoparticulas. La nanotecnología es una tecnología relativamente nueva que se basa en la manipulación de materia a escala nanométrica (Un nanómetro es la millonésima parte de un milímetro), esta tiene usos en la creación de componentes electrónicos, en el envasado de alimentos, etc. Autor:Desconocido Dirección electrónica de la imagen: http://2.bp.blogspot.com/-4kEyCVc4Bx0/VFJ12pAD8\\_I/AAAAAAAADfg/KVthPTF6XKo/s1600/escala%2Bnanometrica.jpg La nanotecnología ha sido posible gracias al desarrollo de las ciencias computacionales, porque es gracias a ellas que se pueden diseñar, analizar y estudiar materiales a escala nanométrica, por ejemplo, con ellas ha sido posible descifrar muchas cosas del genoma humano y con inteligencia artificial poder aprender de manera automatizada con base a patrones para detectar enfermedades; La creación de nanorobots que fue algo nunca imaginado en el pasado también ha sido posible gracias al avance de las ciencias de computación. Los nanomateriales (Imagen 1) tienen propiedades especiales que no tienen materiales del mismo tipo, pero a diferente escala, por ejemplo en un material a escala nanométrica las propiedades eléctricas son modificadas haciendo que estas estructuras sean de entre semiconductoras a superconductoras; las propiedades mecánicas también son afectadas haciendo que las estructuras sean más fuertes y por ultimo las propiedades térmicas haciendo que resistan temperaturas más altas. Autor:Desconocido Dirección electrónica de la imagen: https://nanova.org/wp-content/uploads/ejemplos-nanomateriales.jpg La nanotecnología en la medicina esta comenzado, y por las propiedades de los materiales nanométricos se tiene un futuro prometedor debido a que esta sería capaz de diagnosticar y prevenir enfermedades, así como también de curarlas a nivel molecular y celular. Una de las cuestiones más destacable es que entre los nanomateriales se encuentra el carbono y la aplicación de este sobre el cuerpo humano generaría menos toxicidad que otros materiales. Las ventajas fundamentales de las nanopartículas son: mejor administración de fármacos insolubles en agua, administración dirigida, co-entrega de dos o más fármacos para la terapia combinada, y visualización del sitio de administración del fármaco mediante la combinación del sistema de imagenología y un fármaco terapéutico. Otra ventaja del uso de la nanotecnología en la medicina es que las nanopartículas son biocompatibles y biodegradables y están compuestas de un núcleo, una partícula que actúa como portador y uno o más grupos funcionales sobre el núcleo que se dirigen a lugares específicos. La nanotecnología en la administración de fármacos incluye nanodiscos, nanoestructuras de lipoproteínas de alta densidad, liposomas y nanopartículas de oro. 1 Actualmente se realizan investigaciones de como poder curar y diagnosticar enfermedades como el cáncer por medio de la nanotecnología, porque podría mejorarse la solubilidad y biodisponibilidad de fármacos que son pobremente solubles, además de que se reducirían los efectos secundarios de las terapias anticancerígenas; Por ejemplo, recientemente científicos israelíes del Instituto Tecnológico de Israel (Technion) han logrado exitosamente erradicar un tumor canceroso en ratones (Imagen 3), combatiendo las células malignas del tumor usando una “nano-fábrica”, que es una célula sintética que produce proteínas anticancerígenas dentro del tejido. Según lo comentado por dichos científicos en un comunicado acerca de la operación médica, las células sintéticas “son sistemas artificiales con capacidades similares y, a veces, superiores a las de las células naturales debido a que, así como las células humanas pueden generar una variedad de moléculas biológicas, las células sintéticas pueden producir una amplia gama de proteínas”. Además, agregan que las células sintéticas también “son sistemas que tienen un gran potencial en la disciplina de ingeniería de tejidos, en la producción de órganos artificiales y en el estudio de los orígenes de la vida”. 2 En la actualidad existen muchas investigaciones realizadas con la nanotecnología, como la aplicación de nanotubos (Imagen 2) para regeneración de los huesos o para la mejora de la conectividad en las neuronas, situaciones que hasta hace poco se encontraban solo en la ciencia ficción, por lo que esta tecnología se considera revolucionaria. Autor:Desconocido Dirección electrónica de la imagen: https://nanotecnologia.fundaciontelefonica.com/wp-content/uploads/2013/10/carbon-nanotubes.png El mayor logro que tendrá la nanomedicina es la realización de tratamientos nanorobóticos en células individuales y específicas del cuerpo humano. Los nanorobots son máquinas a escala nanométrica que en la nanomedicina pueden ser usados para identificar y destruir células cancerígenas. Los nanorobots no son los robots convencionales que describe Isaac Asimov, quien formulo las tres leyes de la robótica, debido a que los nanorobots son básicamente partículas de diferentes materiales que desencadenan acciones ante determinadas sustancias, de esta manera dependiendo de la cantidad de sustancias y el orden que lleven se puede tener una secuencialidad de acciones específicas para cumplir una tarea. Los nanorobots en la nanomedicina están basados en el ADN porque las propiedades de la estructura del ADN pueden ser utilizadas como contenedores de agentes terapéuticos que se liberaran una vez se llega a una diana (Objetivo órgano o célula). Un ejemplo del uso de los nanorobots fue desarrollado por los científicos de la Academia China de Ciencias y la Universidad del Estado de Arizona, en donde los nanorobots se usaron para combatir tumores cancerígenos en animales; En este caso se programaron los nanorobots en función de eliminar el flujo sanguíneo que llega al tumor para bloquear el ingreso de nutrientes y así matarlo (Imagen 4). Para realizar esto los nanorobots debían seguir el siguiente algoritmo: Viajar por el flujo sanguíneo. Reconocer los vasos sanguíneos que llegan al tumor. Diferenciar las células cancerígenas de las células normales. Liberar un agente coagulante al llegar a su destino. Autor: Desconocido Dirección electrónica de la imagen: Enlace Para realizar este algoritmo el equipo programo los nanorobots constituyéndolos por una lámina rectangular de ADN que fue doblada para formar nanotubos y que en su interior contendrían cuatro moléculas de trombina, una enzima coagulante. Por último, los nanorobots deben saber identificar su destino, es decir los tumores, de forma que cuando lleguen allí puedan activarse y realizar su trabajo; Para realizar esto se agregaron a las láminas aptámeros de ADN que son moléculas que reconocen la proteína nucleolina que es producida por los tumores en grandes cantidades (Imagen 5); De manera que la nucleolina sirva para activar los nanorobots que al detectarla su lamina regresa a su forma rectangular liberando las moléculas de trombina para iniciar el proceso de coagulación y parar el flujo de sangre al tumor. Luego de aplicar el tratamiento, se verifico que el proceso de coagulación comenzó en un periodo de 24 horas a la vez que los nanorobots fueron desechados del cuerpo de manera natural casi en su totalidad en ese mismo periodo de tiempo, y el proceso de coagulación en los vasos sanguíneos del tumor finalizo en 3 días sin mostrar algún síntoma de toxicidad. Autor: Desconocido Dirección electrónica de la imagen: Enlace Comentario Personal: El uso de la nanotecnología en la medicina es algo trascendental en la historia de la humanidad, porque por primera vez el ser humano puede interactuar de forma molecular con células y moléculas específicas de manera individual, el hecho de programar nanorobots para que funcionen utilizando las leyes de la física y la química es algo impresionante pues sale de lo convencional en términos de lo que se conoce como programación. A la vez es importante resaltar que sin la existencia, desarrollo y avances de las ciencias de la computación la nanotecnología y por ende la nanomedicina no fueran posibles. La nanomedicina es un campo muy joven, pero con un potencial enorme, esta cambiará la manera en que vivimos, haciendo que la calidad de vida de las personas mejore exponencialmente, además de que nos hará comprender mejor la forma en que funciona la máquina del cuerpo humano. Conclusiones: El traslado de ciertos materiales a escalas nanométricas potencia las propiedades de dichos materiales. La nanotecnología es un campo relativamente nuevo, que poco a poco se introduciendo en diferentes disciplinas como la electrónica o la medicina. La nanotecnología y por tanto la nanomedicina son una causalidad del desarrollo y avances de las ciencias de la computación, pues es gracias a ellas que es posible desarrollar, diseñar y analizar materiales a escalas de nanométricas. La nanomedicina será capaz de diagnosticar y prevenir enfermedades, así como también de curarlas a nivel molecular y celular. Los nanorobots son máquinas a escala nanométrica que están diseñadas para realizar una pequeña y especifica tarea. Las células sintéticas son pequeños sistemas a escala nanométrica que tienen las propiedades de una célula natural más otras propiedades, dependiendo de su diseño. La programación de los nanorobots en la nanomedicina es basada en simular el comportamiento de los anticuerpos en los procesos naturales de curación; Y esta programación está en función de ejecutarse siguiendo las leyes de la física y la química. Referencias: 1 Nanova 2 Tumor en Israel 3 https://revistageneticamedica.com/2018/02/15/nanorobots-de-adn/ 4 Tendencias de software de aprendizaje 5 http://www.understandingnano.com/medicine.html "],
["08_esahuay.html", "El valor de los datos en las PYMES", " El valor de los datos en las PYMES Ariel Eliseo Sahuay Velásquez eliseosahuay@gmail.com Estudiante de Ingeniería en Ciencias y Sistemas - USAC Palabras clave: Sistemas de información, Business Intelligence, BI, Valor de la información, PYME, pequeña y mediana empresa Como se ha dicho según las empresas implementan distintos sistemas para la gestión de sus datos, pero será la forma adecuada como manejan sus datos, esto surge debido a que se tenga un sistema grande de información y complejo con una interfaz con muchos colores y gráficas creadas con la mejor tecnología en el mercado no es necesariamente efectivo. O que se tenga un sistema pequeño con 4 reportes. Todo esto depende de la información que reflejan los reportes y los datos almacenados. No importa cuán grande o pequeño sea el sistema si no que la información presentada sea obtenida de forma rápida, fácil y útil para lo que estemos analizando, porque el tener un sistema que nos provea información útil y rápida nos puede ayudar a una toma de decisiones, un control de ventas, un control de ingresos y gastos de las empresas. Así como poder visualizar los puntos picos en los cuales se venden más productos o cuando tenemos menor demanda. Todo esto y mucho más podemos saber con la información en nuestro poder. Cosa que muchas veces los gerentes o dueños de las pequeñas y medianas empresas desconocen, de lo que tienen en su poder, en aquellos archivos de Excel, Word, libros contables, etc. Que esta información puede ser extraída, transformada y cargada (ETL) en una herramienta de BI, en la cual se puede analizar ventas, compras y encontrar una respuesta a problemas sobre cuando abastecer y cuando no de materia prima. De cuántos recursos necesitamos en una temporada de producción, si los meses que se aproximan son altos o bajos en ventas, si necesitamos invertir en máquinas de producción o si necesitamos hacer cambios. Esto lo podemos saber analizando todos los datos históricos almacenados en aquellos archivos de Excel de 40Mb de una empresa, en donde anotan las ventas y pedidos. Por medio de herramientas de Business Intelligence si los datos al menos están estructurados. Caso contrario sería otra la forma de abordar este problema. Muchas veces se piensa que el implementar un sistema de manejo de datos y más un sistema de análisis de datos son gastos innecesarios y que no traen ventaja alguna. En el caso de los sistemas de información hay empresas que aún utilizan archivos de Word y Excel para llevar el control de cuentas, inventarios, ventas y compras. No siendo la mejor opción ya que existe un problema de escalabilidad si se utiliza este método. El acceder y buscar en ellos cada vez se hará un poco más tardado, tedioso y repetitivo, el tener un sistema de información que administre toda la data importante para la empresa, hará tener un medio más seguro, ordenado y controlado. Más fácil de manipular e ingresar información a la base de datos la cual alimentaria al sistema de análisis de datos. En el caso del análisis de datos o Business Intelligence que, en pocas palabras, es el análisis de nuestros datos históricos para una mejor toma de decisiones, existen soluciones que son de Open Source que reducen costos de inversión, también empresas grandes como Microsoft ofrece soluciones a un precio relativamente bajo. Si pensamos solamente en la solución esa sería la respuesta. Dentro de los beneficios que conlleva implementar sistemas de información se pueden mencionar la mejora de la toma de decisiones, además de esto se pueden reducir costos y si se hace un uso adecuado de toda esta información la inversión se puede recuperar rápidamente, ya que conoceremos nuestro estado actual, definir a donde queremos llegar y tomar mejores decisiones para alcanzar nuestro objetivo. Para ejemplificar el valor de los datos podemos ver el caso de Tesco. Tesco inicio como una línea de supermercados, fue de las primeras compañías en experimentar los beneficios de implementar Business Intelligence y el aprovechamiento de los datos generados, para poder implementarlo inicio promoviendo membresías. Con ellas pudo obtener los datos y observar los patrones de comportamiento de sus clientes. Con toda la información recolectada, se pudo observar las ventas históricas y predecir el stock necesario, el éxito fue tal que llegaron a predecir las ventas por producto en cada tienda, siendo capaz de ahorrar 100 millones de libras. El implementar BI se puede realizar en distintas industrias tal es el caso del Hotel Wellington donde su mayor problema era el tiempo y costo que consumían el poder generar reportes de las situaciones en el momento, debido a esto busco centralizar la información de sus procesos y tener una solución que le diera los reportes en poco tiempo, costo relativamente bajo, explorar la información presentada. “El valor de la información es cero, es lo que haces con ella lo que realmente importa”2 Conclusiones El buen uso de la información nos puede dar una ventaja considerable sobre qué hacer y qué no hacer en determinado momento. La información es conocimiento y el conocimiento nos hará tomar la mejor decisión sobre donde estamos, a dónde queremos llegar y pensar en la estrategia correcta para llegar a nuestro objetivo. Un sistema de información es exitoso, si la información que almacena es útil y aporta lo que realmente se necesita para la empresa. La inversión inicial de un sistema de BI se recupera según el nivel de utilización que se dé a los reportes e información proporcionada. Una empresa que no conoce el valor de la información que tiene en su poder desconoce dónde está y hacia dónde se dirige. Referencias Pau Urquizu (23/12/2011). Consejos para la PYME que busca Business Intelligence. (10/2/2018) Laemoyelemo (12/9/2010). Era de la información. (10/2/2018) Solo Pienso en TIC (24/10/2017). Business Intelligence: los casos de éxito más famosos. (10/2/2018) Inteligencia de Negocios (26/09/2013) Tres casos de Éxito de B.I. (24/09/2018) "],
["09_jbaldelomar.html", "Seguridad Informática: Encriptación de la Información", " Seguridad Informática: Encriptación de la Información Juan Luis Baldelomar jlbc95@gmail.com Estudiante de Ingeniería en Ciencias y Sistemas - USAC Palabras clave: Encrypt, private key, public key, Diffie-Hellman, RSA, AES, Symmetric Cryptography, Asymmetric Cryptography. El crecimiento de la tecnología en el área de la computación y comunicaciones ha presentado un sesgo exponencial. Hace algunas décadas se consideraba imposible tener un computador en una casa debido a su gran tamaño y costo, ahora es posible tener una computadora mucho más poderosa que una supercomputadora de esa época en una mano, conocido actualmente como Smartphone. Esto nos puede dar una idea de que tan rápido ha avanzado la tecnología en esos campos. Debido a estos avances, las tecnologías de comunicación y computación han sido utilizadas para acelerar el procesamiento de grandes cantidades de información y optimizar la forma en como esta información se transmite de un punto a otro. Este paso de información muchas veces se hace a través de una red (puede ser llevado a cabo a través de un dispositivo de memoria físico, a través de otra tecnología como bluetooth, etc…) ya sea interna o externa. Aquí es donde surge un problema, ¿Cómo se garantiza que en este paso de información de un punto a otro, un tercero que se encuentre husmeando no pueda tener acceso a esta información? Uno podría pensar “¿A quién le puede interesar lo que yo hablo con otra persona?”, pero la realidad es que la información el día de hoy tiene un gran valor para quienes saben qué hacer con ella. Las empresas pagarían miles para poder tener acceso a los libros de contabilidad de su competencia, empresas que venden servicios y productos pagan miles para saber qué quieren la mayoría de personas y obtener así más clientes, incluso un escenario más concreto es el caso que Edward Snowden1 dio a conocer en el 2013 de como la NSA espiaba a ciudadanos estadounidenses y de todo el mundo para tener un mejor control a nivel mundial. La solución a este problema no es únicamente evitar que algún tercero obtenga esta información, sino también considerar el caso en que pueda obtenerla pero no entenderla y a esto es lo que se le conoce como encriptación. La encriptación es una técnica utilizada como medida de seguridad para la transmisión de información que se basa en la criptografía. La criptografía es una ciencia que aplica complejos conceptos de matemática y lógica para ocultar el significado de un mensaje. No se debe confundir la encriptación con la esteganografía. Esta última consiste en ocultar un mensaje en algún objeto de manera que este pase inadvertido. La encriptación, en cambio, consiste en volver un mensaje ilegible mediante una clave de tal manera que solo se pueda volver legible mediante el proceso inverso con la clave correcta. Es decir, el mensaje puede ser visto por alguien ajeno, pero inentendible. En la imagen 1 podemos observar que tenemos 3 participantes. Andrea y Carlos, que son quienes desean comunicarse, y Jose quien desea husmear en la conversación. A ese escenario se le conoce como “Man in the Middle Attack”. Se debe comprender que Andrea y Carlos pueden ser dos personas que se desean comunicar a través de la red, o bien Carlos puede ser un servidor que provee algún servicio a través de la red como una cuenta de banca online; cual sea el caso, la información que es enviada a través en dicha comunicación debe ser transmitida de forma segura. Imagen 1 Autor: Imagen Propia Inicios de la Criptografía La criptografía nace debido a la guerra. Hace cientos de años los imperios necesitaban información para poder entrar en un conflicto bélico y poder planear estrategias de conquista y prevención. Esto dio origen a la necesidad de esconder el significado de esa información para que espías e infiltrados enemigos no pudieran comprenderla. Así mismo, también dio origen a la búsqueda de técnicas para poder romper esta medida de seguridad y así poder tomar ventaja sobre sus enemigos. Los criptógrafos de esa época utilizaban 3 métodos para encriptar la información: sustitución, transposición y códigos. No entraremos en detalles de estos métodos porque no es el objetivo principal de este artículo discutir toda la historia de la criptografía. Criptografía Contemporánea La criptografía ha tenido muchos avances desde sus inicios. Diversos algoritmos de cifrado han sido inventados; tras ser vulnerados se llegó a la conclusión de que un buen algoritmo de cifrado debe hacer imposible encontrar el texto original sin el conocimiento de la clave. Los sistemas de criptografía hoy en día se pueden dividir en tres principalmente: simétricos, asimétricos e híbridos. Cada uno posee sus características positivas y negativas. Se podría decir que técnicamente un mensaje encriptado con uno de estos sistemas no es imposible de descifrar, pero esto aplica únicamente en el ámbito teórico debido a que llevarlo a la práctica conlleva el uso de una enorme cantidad de recursos y tiempo. Si abordamos el problema desde el punto de vista de recursos, se llega a la conclusión que actualmente todavía no se cuenta con el poder de procesamiento para vulnerar uno de esos sistemas, y si hablamos de tiempo se encuentra que para vulnerar un mensaje encriptado con uno de los algoritmos utilizados hoy en día se requiere de más tiempo que el tiempo de vida del universo observable. Criptografía Simétrica La criptografía simétrica se basa en el uso de una única clave para encriptar y desencriptar un mensaje. Esta clave debe ser conocida por el emisor y el receptor en un proceso de comunicación para que ambas tareas puedan ser logradas, de lo contrario el receptor no podría desencriptar el mensaje, y es aquí donde se encuentra la debilidad de este proceso. ¿Cómo garantizar que la clave no sea interceptada? De este punto surgen formas ingeniosas que tratan de dar solución a esta problemática como la criptografía híbrida. Ahora bien, en cuanto a encriptación de datos locales se refiere no cabe duda alguna que este sería el método adecuado por su fuerte seguridad y rapidez. Criptografía Asimétrica Se pudo observar que la criptografía simétrica tiene una debilidad en cuanto a establecer una comunicación; dado que el mensaje se encripta y desencripta con una misma clave, esta debe ser enviada al receptor, y si la clave es transmitida por un canal inseguro puede ser interceptada, lo que haría inútil nuestro algoritmo de encriptación. Entonces ¿Cómo poder asegurar que un mensaje sea encriptado y desencriptado sin tener que enviar la clave? Aquí es donde nace la criptografía asimétrica. También conocida como criptografía de clave pública, la criptografía asimétrica nace en la década de 1970 en donde saber cómo enviar un mensaje encriptado no ayuda a saber cómo desencriptarlo. El sistema funciona con dos claves, la clave pública y la clave privada. La clave pública es utilizada para encriptar los mensajes y es de conocimiento de todos los que puedan observar la conversación, por eso es pública. La clave privada en cambio es utilizada para desencriptar los mensajes y únicamente el receptor debe conocerla. Cada sujeto que interactúa en la conversación tiene ambas llaves y son únicas. Estas dos claves poseen alguna relación matemática compleja y difícil de computar; por lo tanto el conocimiento de la llave pública no permite a alguien recuperar el mensaje original sin un ataque de fuerza bruta. La desventaja de los algoritmos que implementan criptografía asimétrica es que son más lentos y un poco más vulnerables a estos ataques de fuerza bruta si las claves que se utilizan para encriptar son relativamente pequeñas (100 dígitos). Criptografía Híbrida {#criptografía-híbrida} Ahora que ya se vio un poco de ambos acercamientos, criptografía simétrica y asimétrica, analicemos sus características positivas y negativas. En el caso de la criptografía simétrica su fuerte seguridad y resistencia a ataques de fuerza bruta y además su rapidez son aspectos positivos, pero requiere de una misma llave para encriptar y desencriptar el mensaje y esta puede no ser transmitida por un canal seguro lo cual resulta siendo un aspecto negativo. En el caso de la criptografía asimétrica se utiliza dos claves, una pública y una privada, por lo cual no es necesario utilizar la misma clave para desencriptar el mensaje y esta no debe ser transmitida con el mensaje y eso resulta ser algo positivo, pero es considerablemente más lento y más vulnerable a ataques de fuerza bruta considerando que la factorización de números primos muy grandes es un área que avanza más y más cada vez. En consideración de esto surge otro acercamiento, la criptografía híbrida. La criptografía híbrida, como su nombre lo puede delatar, es la mezcla de los dos acercamientos anteriores, en donde se utiliza algún algoritmo de criptografía simétrica, como AES, para encriptar el mensaje y se utiliza algún algoritmo de criptografía asimétrica, como RSA, para encriptar la llave del algoritmo simétrico y transmitirla por un canal o medio que no es seguro. Protocolos de Encriptación {#protocolos-de-encriptación} Se ha hablado de como la criptografía puede ser utilizada para volver seguro un mensaje, y como puede ser utilizada de forma general en el intercambio de mensajes, sin embargo eso no es suficiente para la transmisión de información de manera segura y esto se consigue con los protocolos de Encriptación. Hablaremos de como la criptografía puede ser utilizada para el intercambio de una clave secreta por medio de un canal de comunicación inseguro. Diffie-Hellman Key Exchange El protocolo de intercambio de clave Diffie-Hellman es un protocolo que dos sujetos pueden utilizar para compartir una llave secreta por un canal de comunicación inseguro sin haber compartido nada de información en el pasado. Generar una clave secreta compartida entre dos personas es algo de mucha utilidad. Se podría utilizar como clave privada de un sistema de criptografía simétrica. El protocolo fue descrito por Whitfield Diffie y Martin Hellman en 1976. Autor: Propia Si n es lo suficientemente grande, como un número de 300 dígitos, y también lo son a y b, como un número de 100 dígitos cada uno, el sistema se considera totalmente seguro contra ataques de fuerza bruta con el poder de cómputo con el que se cuenta actualmente. Imagen 2 Autor: Wikipedia User Dirección Electrónica de la Imagen: https://en.wikipedia.org/wiki/Public-key\\_cryptography Recomendaciones Cuando se desea transmitir información privada por un canal de comunicación inseguro, aún si nosotros no la consideramos importante, debemos encriptar le información. Si se desea encriptar la información para almacenarla de manera local se recomienda utilizar el algoritmo AES por su seguridad y rapidez. Si se desea enviar información por un canal de comunicación inseguro se recomienda utilizar un algoritmo asimétrico como RSA o un método híbrido, encriptando los datos con el algoritmo simétrico y encriptando la llave de este con un algoritmo asimétrico. Conclusiones La criptografía es la ciencia que se encarga de aplicar conceptos matemáticos y lógicos para ocultar el sentido de la información. La criptografía simétrica es altamente segura y rápida para la encriptación de datos, pero el uso de una misma clave para encritpar y desencriptar la información es un punto vulnerable. La criptografía asimétrica utiliza dos claves, una pública y una privada para encriptar y desencriptar la información respectivamente, pero el proceso que conlleva es más lento. La encriptación de la seguridad es solo una de las herramientas de seguridad informática y no debe tomarse a la ligera ninguno de los otros aspectos que este campo abarca solo por tener la información encriptada. Referencias “Chapter 7: The Role of Cryptography in Information Security” Disponible en: Enlace \\[Consultado: 28/01/2018\\] “Advanced Encryption Standar” Disponible en: http://nvlpubs.nist.gov/nistpubs/FIPS/NIST.FIPS.197.pdf \\[Consultado: 30/01/2018\\] Kenneth H. Rosen, Discrete Mathematics and Its Applications, 7th Edition, McGraw-Hill, Monmouth University 2012, 1071 pp. Edward Snowden es un ciudadano estadounidense de 34 años que trabajó para varias agencias de inteligencia. Es reconocido principalmente en la actualidad por los documentos que libero en el año 2013 que detallaban como estas agencias espiaban a ciudadanos de varios países. Actualmente reside en Rusia. Referencias: https://www.edwardsnowden.com/ https://www.theguardian.com/us-news/2017/jan/18/edward-snowden-allowed-to-stay-in-russia-for-a-couple-of-years↩ "],
["10_echopen.html", "SAP una visión de Negocio", " SAP una visión de Negocio Edgar Mikhail Chopen Sapon mikhailchopen@gmail.com Estudiante de Ingeniería en Ciencias y Sistemas - USAC Palabras clave: ERP, SAP, Software de administración de recursos. En una época donde el modelo de negocio y alcance operativo era definido y estructurado por la capacidad del personal, sucursales y sistemas computacionales no flexibles ni escalables, una barrera de tecnología e infraestructura que, si bien permitió a las empresas alcanzar un capital y aumentar su área operativa, implicaba aumentar proporcionalmente la cantidad de empleados y sucursales, dificultando la minería de datos en busca de un patrón de mercado, siendo no suficiente para un modelo de negocio globalizado, recursos y control de mercado. Y es de esta necesidad que nacen las herramientas ERP (Enterprise Resource Planning/sistemas de planificación de recursos empresariales), en el cual SAP oriento su ERP a servicios y no solo para producción e inventario como antiguamente era un ERP. SAP abarca áreas de logística, ventas, contabilidad, RRHH, entre otros, optimizando los recursos en las actividades. SAP ORIGEN Fue desarrollado por un grupo de 5 ex trabajadores de IBM, dicho programa inicio como una pequeña compañía alemana en la fecha de 1972, su nombre proviene de Systems, Applications &amp; Products por su nombre en inglés. Los fundadores tenían la visión de crear un programa el cual permitiera que los datos fuesen accesibles en tiempo real por los usuarios durante los procesos operativos, estando por encima de los tradicionales archivos para sincronizar las sucursales. Con el cual nace el ERP de SAP, el cual tiene como alcance controlar y planificar todos los recursos empresariales en la gestión de cada proceso interno, en las áreas de planeación, producción, ventas, logística, RRHH, entre otros. El producto R/3 de SAP es un conjunto de funcionalidades las cuales integran los procesos organizacionales, para lo cual trabaja en tiempo real los 3 niveles de interfaz, aplicación y base de datos, que son las 3 capas que describen la arquitectura cliente-servidor del sistema R/3 SAP. SAP ofrece distintos productos los cuales están diseñados para todo tipo y tamaño de empresa, con lo que aseguran ajustarse a todo tipo de procesos y su continuo cambio. SAP BUSINESS ONE Es una herramienta completa he integrada con una interfaz similar a Windows, la cual ha sido desarrollada específicamente para empresas pequeñas y medianas, con menos de 100 empleados y 30 usuarios que buscan cubrir las áreas de negocio. Se distingue por su simple navegación y opciones de expansión, con su innovación de funciones Drag&amp;Related y su integración con Microsoft Word y Excel. Esta versión permite administrar las áreas más importantes de un negocio como ventas, distribución y finanzas, en tiempo real. SAP BUSINESS BYDESING Es la última solución para empresas pequeñas y medianas entre 100 y 500 empleados, los cuales requieren de una solución y demanda de procesos que permita al mismo tiempo los beneficios de menores costos operativos, con la ventaja de integrar aplicaciones de punto a punto con el menor riesgo, garantizando las bases de una empresas enfocada y diseñada a una arquitectura de servicios. SAP BUSINESS SUITE Es la versión más completa de los productos SAP, que permite administrar completamente cada área de una empresa y sus puntos operativos. SAP Business Suite provee un espectro completo de soluciones de negocio, una infraestructura alta mente integra e interfaces para integración de productos No SAP. Autor: SAP TAW10_1 ABAP MÓDULOS es un sistema ERP, el cual permite manejar todas las áreas de una empresa, esto a todos sus módulos MM, SD, DI, PP y PM. Modulo MM (Materials Management) principal modulo del ERP SAP y está diseñado para la logística de los productos, manejando stock y el movimiento de estos, es decir planificación, almacenamiento y compras. Modulo SD (Sales &amp; Distribution) Diseñado para los procesos de ventas y entregas los cuales se relacionan con un cliente, también entra en la categoría de módulo de logística. Modulo PP (Production Planning) modulo en el cual se planifica todo lo relacionado al producto, en lo principal las plantas de fabricación esto en un mediano y largo plazo, lo cual constituye un MRP. Modulo CO (Controlling) Modulo para el control, el cual permite analizar los KPI respecto a la gestión interna en la empresa, relacionando costos y beneficios. LENGUAJE DE PROGRAMACIÓN SAP Cuenta con un lenguaje de programación interpretado para el desarrollo de aplicaciones en su sistema SAP R/3, llamado ABAP, el cual le permite desarrollar aplicaciones en la mayoría de los productos de SAP y utilizar cualquier base de datos a través de sus sentencias de OPEN SQL. Esto hace de ABAP un lenguaje 4GL (de cuarta generación). Si bien ABAP es un lenguaje de programación, este esta limitado a la creación de software para mejora, variaciones y nuevos procesos internos del negocio. Si bien no permite una interacción directa con usuarios finales de mercado, debido a su alto encapsulamiento para asegurar la seguridad de la información, permite la creación de RFC (Remote Function Call) las cuales proveen una interfaz de comunicación entre sistemas SAP, como lo sería un POS (Point OF Sale) SAP o PO (Process Orchestration) SAP como bus de integración, el cual permitiría la comunicación con sistemas No SAP. Autor: SAP TAW10_1 ABAP Programa ABAP A un inicio SAP integro su lenguaje ABAP con una estructura Top-Down no orientada a objetos, que posteriormente fue orientado a objetos. La impresión de los resultados de un programa ABAP tiene distintos métodos, desde el más antiguo como lo sería un SAPSCRIPT, posterior un ALV permitiendo mayor interactividad al usuario, hasta reportes mas modernos que permite un mejor diseño de los mismos, como lo seria un SmartForm o un Adobe Form. Código \"Nombre del programa, la z al inicio indica que es desarrollado por un usuario report ZImprimirClientes. \"KNA1 es la tabla maestra de clientes tables: KNA1. \"Es un select que integra un loop en la información devuelta SELECT * FROM KNA1 ORDER BY name1. \"imprime el nombre y teléfono del cliente WRITE: / KNA1-name1, ’ ’, KNA1-telf1. ENDSELECT. Conclusiones En conclusión, SAP ERP abarca todo tipo de mercado y tamaño de empresa, ofreciendo una estructura orientada al servicio. Por tanto, permite un control mas optimo sobre los procesos, gracias a su acceso en tiempo real. Si bien, la implementación de un sistema ERP SAP, incurre en altos costos, como la misma implementación e integración de un personal cualificado, SAP asegura minimizar los costos operativos aumentando las ganancias. Referencias TAW10_1 ABAP workbench Fundamentals, SAP AG. WWW.SAP.COM Autor: SAP - https://darsisit.com/sap/glosario-sap/ "],
["11_agutierrez.html", "5 convenciones de código para una aplicación Android", " 5 convenciones de código para una aplicación Android Mario Alexander Gutierrez Hernández alex.dev502@gmail.com Estudiante de Ingeniería en Ciencias y Sistemas - USAC Palabras clave: Metodología, codigo fuente, java, Android Al trabajar una aplicación Android en un grupo de trabajo nos topamos, muchas veces, con archivos que nos cuesta entender, variables con nombres extraños, funciones que devuelven cosas raras, codigo sin identacion, etc. Estos problemas generar un codigo que es dificil de entender para el equipo de trabajo, incluso para el mismo desarrollador que lo escribio, a lo largo del proyecto Dirección: Enlace Estandarizar la escritura del codigo fuente en un proyecto en una aplicación Android, y en proyectos en general, hace que el equipo de trabajo pueda navegar por los diferentes archivos sin ningun inconveniente para comprenderlos. Es por eso que recomiendo 5 convenciones en Java y Android para escribir nuestro codigo fuente. 1. Nombres de clases Los nombres de las clases deben ser sustantivos, cuando son compuestos tendrán la primera letra de cada palabra que lo forma en mayúsculas (UpperCamelCase). Intentar mantener los nombres de las clases simples y descriptivos. Usar palabras completas, evitar acrónimos y abreviaturas. Ejemplo: class Carro class DocumentoPersonal 2. Variables Los nombres de las variables deben ser cortos pero con significado. La elección del nombre de una variable debe ser un mnemónico, designado para indicar a un observador casual su función. Los nombres de variables de un solo carácter se deben evitar, excepto para variables índices temporales. Se debe utilizar la forma de escritura lowerCamelCase, la primera letra minúscula seguido de la primera letra mayúscula de cada palabra. -Variables globales no publicas y no estáticas deben iniciar con “m”. Ejemplo: private int mTotal; protected String mNombre; -Variables estáticas deben iniciar con “s” Ejemplo: private static Repositorio sRepositorioTrabajo; -El resto de las variables simplemente utilizan lowerCamelCase Dirección: Enlace 3. Constantes Las constantes son valores que no cambian en toda la aplicación, su nombre tiene que ser significativo y con todas sus letras en mayúsculas, separando las palabras con guiones bajos. Ejemplo: publc static final int UN_VALOR = 1; 4. Métodos Los nombres de los métodos deben de ser significativos en base a la acción que realizan, su escritura se debe hacer utilizando el estándar lowerCamelCase. En cuanto a la longitud de contenido de un método mantenga los métodos pequeños y enfocados. Muchas veces los métodos son de contenido largo y se comprende, por lo que no se establece un límite estricto en la longitud del método. Si un método supera las 40 líneas, hay que considerar si se puede dividir sin dañar la estructura del programa. 5. Comentarios Comentar el código es de las cosas mas importantes que se deben hacer, los comentarios deben contener información precisa de los componentes mas importantes de cada clase, como mínimo. La forma de comentar el código es utilizar el estándar Javadoc, con este estándar podemos generar incluso un documento con toda la información agregada en los comentarios de nuestro proyecto (clases, métodos, atributos…). Poniendo en practica los 5 puntos mencionados, empezaremos a ver como el código fuente de la aplicación es cada vez mas legible para el equipo de trabajo, logrando así incluso un avance mas rápido en el desarrollo del proyecto, ya que a cada uno de los programadores se le facilita entender la función de cada porción de código, escrita por cada uno de sus compañeros, si la llegara a necesitar. Conclusiones: Las convenciones de codigo hacen un codigo mas legible para los programadores, tanto actuales como futuros, del proyecto. Aprender estandares de programacion no solo nos ayudan a trabajar en proyectos locales sino tambien en proyectos globales, que se trabajan en comunidad. Referencias: https://source.android.com/setup/code-style Scott Hommel, traducido al castellano por Alberto Molpeceres. Convenciones de codigo para el el lenguaje de programacion JAVA. http://www.um.es/docencia/vjimenez/ficheros/practicas/ConvencionesCodigoJava.pdf "],
["12_lleal.html", "NALU(unidad aritmetica lógica neuronal) – Su razón de ser", " NALU(unidad aritmetica lógica neuronal) – Su razón de ser Ing. Luis Fernando Leal wichofer89@gmail.com Invitado especial Palabras clave: NALU, inteligencía artificial, Memorización en IA Este post planeo dividirlo en 2 partes ,1(el actual) explicando las motivaciones de NALU y el problema que busca resolver y en el próximo un poco mas de detalles técnicos , explicación del modelo , código de mi propia implementación del modelo(los autores solo publicaron las especificaciones, pero no su código) y resultados de algunos de mis experimentos. NALU es un nuevo modelo propuesto por Google DeepMind, siendo su autor principal Andrew Trask(autor del libro “Grokking Deep Learning”) y publicado apenas hace 2 semanas(agosto 2018) como artículo de investigación científica en arxiv https://arxiv.org/pdf/1808.00508v1.pdf NALU busca mejorar el rendimiento de los modelos de redes neuronales y abrir la puerta a nuevos modelos y nuevas aplicaciones no antes posibles ya que busca solucionar un importante problema presente en las redes neuronales : falta de razonamiento cuantitativo y la habilidad de extrapolar y generalizar La habilidad de representar y manipular cantidades numéricas esta presente en muchas especies, no solo en los humanos, por ejemplo hay fuerte evidencia que indica que las abejas tienen la habilidad de contar objetos y razonar sobre cantidades numéricas, esto sugiere que la habilidad de representar y manipular cantidades numéricas es un componente inherente de la inteligencia biológica,entonces la pregunta es : ¿posee la inteligencia artificial actual esta habilidad? La respuesta lamentablemente es NO, las redes neuronales actualmente no poseen la habilidad de “razonar” cuantitativamente y manipular cantidades numericas de manera sistematica y generalizada. Alguien con conocimiento del tema puede leer la ultima frase y decir : “esto no es cierto, todo lo contrario de hecho las redes neuronales solo trabajan con números y todo tipo de información que se les alimente debe primero ser convertida a números”,y aun que esto es cierto con algunos ejemplos y analogías quedará mas claro cual es el problema que NALU busca resolver, empecemos entonces con una analogía con el aprendizaje humano: Aprendizaje vs memorización Durante mi tiempo de estudiante en ingeniería adopte por gusto y desición propia como método de estudio de matemáticas el realizar todos los ejercicios contenidos en el libro utilizado como material del curso,esto me consumía bastante tiempo por lo que algunos de mis amigos y compañeros sugerían “no, para ganar el curso solo es necesario aprender los ejemplos vistos en clase y la tarea preparatoria” lo cual obviamente consumía bastante menos tiempo ,muchas veces en los examenes solo venian estos ejemplos o pequeñas variaciones de los mismos, por lo que memorizarlos garantizaba ganar el curso, pero ¿es esto aprendizaje? En otros casos los problemas si eran diferentes y se necesitaba aplicar razonamiento y uso de los conceptos para poderse resolver y esto provocaba que algunos compañeros que preferían el método rápido(memorización) fallaran al no poder generalizar o extrapolar . Este ejemplo sugiere que los humanos podemos tanto aprender como memorizar (y algo mas interesante aún es que tenemos el poder de decicir cual de las 2 cosas hacer,pero ese es otro tema) lamentablemente la IA a través de redes neuronales actualmente esta mas cerca del caso 2(memorizar) que del 1(aprender de manera general). Memorización en IA La IA a través de redes neuronales ha logrado resultados impresionantes en complejas tareas como: Reconocimiento de voz y lenguaje natural Diagnostico y detección de enfermedades Vencer a campeones del mundo en juegos de mesa Reconocimiento facial etc Pero tiene dificultades en tareas mucho mas sencillas como: Contar números Aplicar aritmética simple(suma,resta, multiplicación ,división) Es decir tareas que implican razonamiento numérico ,algo básico de nuestra inteligencia(y de otras especies),por ejemplo cuando nosotros aprendemos a sumar, nuestro cerebro maneja de manera abstracta y conceptual los números y la operación de suma en estos, no memorizamos cual es la suma para cada posible par de números en la recta númerica(sería imposible) . Algunos estudiosos de neurociencias y el cerebro proponen que esto es posible gracias a un tipo especial de neurona llamada “numeron”(por “numerical neuron”) las cuales se encargan de crear las representaciones numéricas que nuestro cerebro usa y proponen que todo razonamiento numérico y operaciones con numeros se basa en operaciones de acumulación(suma y resta) de cantidades(para mayor detalle ver : The Number Sense: How the Mind Creates Mathematics de Stanislas Dehaene), en el siguiente post veremos como este modelo se refleja directamente en NALU Otros ejemplos donde es posible ver este problema en la IA son los siguientes casos: Las redes neuronales son muy buenas aprendiendo funciones f(x) pero tienen dificultad en arender la sencilla función “identidad escalar”, es decir f(x) = x para valores de “x” que no ven durante su entrenamiento. Las redes neuronales en algunos casos pareciera que han aprendido aritmetica basica, por ejemplo aprenden a sumar correctamente pero solo lo hacen bien en el rango de valores vistos durante su entrenamiento(por ejemplo si durante su entrenamiento ven valores en el rango de 0 a 100, podrán sumar bien nuevos valores en ese rango,pero no lo harán bien con valores como 101 + 200) por lo cual decimos que no pueden extrapolar a rangos de valores no vistos durante su entrenamiento. Tuve la oportunidad de “chatear” con el autor del modelo quien en sus propias palabras explica otro ejemplo donde se entrena a una red neuronal para contar hasta cierto numero, pero una vez entrenada tiene dificultades para contar hasta un numero mayor a los observados, esto sería el equivalente a un humano contando muchas veces de uno a 10, pero luego tener dificultades contando hasta 11. Imagen 1 del artículo original: credito a DeepMind (descripción abajo) Imagen 1 del artículo original: en esta imagen se prueban diversas funciones de activación en un autoencoder aproximando la función identidad escalar f(x) = x , cada curva pertenece a una función de activación, el eje \"x\" corresponde a los valores de entrada \"x\" y el eje \"y\" corresponde al error obtenido, es posible ver que para algunos casos pasa en menor medida que otros,pero en todos los casos el error incrementa cuando se sale del rango de valores \"x\" vistos durante el entrenamiento (aproximadamente -5 a 5) El autor explica que la hipótesis bajo la cual se desarrolló este proyecto de investigación fue que las redes neuronales aprenden a representar números(y las operaciones con estos) de la misma manera (o similar) a como aprenden a manipular palabras, es decir: creando un diccionario finito lo cual es una limitación en operaciones numéricas si tomamos en cuenta que en la recta numérica existen infinitos valores y simplemente no podemos representar un diccionario de infinitos elementos en una red neuronal(y aun que pudieramos, ¿sería esto realmente inteligencia?) lo cual limita a la IA actual en extrapolar funciones a rangos de valores no vistos previamente (similar al ejemplo de memorización vs aprendizaje) El objetivo del proyecto era proponer una nueva arquitectura que solventara estas limitantes y permitiera enriquecer a los modelos convencionales (redes feed-forward , convolucionales, recurrentes, etc) extendiendo su habilidad de extrapolar y generalizar en rangos de valores númericos nunca antes vistos…y lo lograron . Aun que NALU tiene un potencial muy grande y creo que será una herramienta importante para todo AI developer, lo que mas me ha gustado es que ha abierto las puertas para nuevos modelos y arquitecturas demostrando que es posible aplicar una estrategia de diseño en la que se agregan a redes neuronales sub-elementos que buscan resolver ciertas dificultades y aplicar de manera efectiva sub-funciones matemáticas (en este caso aritmetica) En el próximo post escribiré un poco mas de los detalles técnicos de NALU y mi experiencia en su implementación , y el experimento en el cual apliqué NALU. De manera anticipada , en el experimento en el cual he aplicado NALU hasta el momento se busca responder la pregunta “¿puede una red neuronal aprender a restar numeros seleccionados aleatoriamente en un vector de entrada x?”, mis experimentos parecen indicar que sí y tal como los autores de NALU buscaban, intente lograrlo en rangos de valores nunca vistos durante el entrenamiento. En el próximo post pondré detalles de este experimento(así como codigo del mismo) Espero con este post explicar un poco mas claro la necesidad y problema que NALU busca resolver ya que muchas veces una publicacion científica es un poco difícil de digerir y con mucha notación matemática y mas aún en latinoamérica donde nuestro idioma nativo no es el ingles,por lo que espero que este artículo sea útil para personas buscando información un poco mas digerible del tema y en su lenguaje nativo. Articulo publicado en: https://devongt.blogspot.com/ "]
]
