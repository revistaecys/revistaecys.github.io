[
["index.html", "Decimocuarta Edición - ECYS Revista Digital Links Disponibles", " Decimocuarta Edición - ECYS Escuela de Ingeniería en Ciencias y Sistemas. 2019-08-01 Revista Digital Links Disponibles Revista Ciencias, Sistemas y Tecnología. Facultad de Ingenieria Escuela de Ingeniería en Ciencias y Sistemas Revista Ciencias, Sistemas y Tecnología - Issuu.com "],
["00-editorial.html", "Editorial", " Editorial Las áreas de desarrollo software son hoy en día una de las áreas mejor remuneradas a nivel global, pero también es una de las más demandantes respecto a la constante actualización. Una de las principales ventajas de ésta es la movilidad que brinda a sus profesionales, los cuales hoy por hoy deben responder a una tendencia globalizada de la demanda del mercado. Eso hace necesario tomar en cuenta varios conceptos tales como multi plataforma, multi servicios, integración, multi dispositivos, etc. Pues deberá buscar la compatibilidad entre los distintos desarrollos. Toda esta lógica hace cambiar su paradigma de formación el cual deberá ir orientado a la formación de profesionales globales los cuales respondan a un entorno globalizado, independientemente de su localidad. Ello significa que podrá brindar servicios e integrar equipos indistintamente de su ubicación geográfica, lo cual brinda una ventana de oportunidades de crecimiento y desarrollo. Pero también se deberá tomar en cuenta que esto implica el desarrollo de otra serie de competencias y habilidades de comunicación, las cuales posibiliten formar parte de ese entorno global, el cual si bien es cierto abre una serie de opciones, también será más demandante respecto a la competencia existente en el medio. Es así como este es el reto del profesional global, el cual tiene cada vez menos limitados sus límites y cada vez mayores retos. MSa. Ing. Carlos G. Alonzo Director de Escuela de Ingeniería en Ciencias y Sistemas – USAC "],
["01-contenido.html", "Contenido", " Contenido 01 Introduciendo Firebase 21 El conocimiento no se destruye se transforma 04 Edge Computing 23 Moderación de contenido, aplicación práctica de funciones hash 07 Flutter: el presente de las aplicaciones móviles 26 UI/UX - El diseño dentro del software 10 Optimización de sitios web con Google Analytics 28 Blockchains, un futuro cercano 14 Computación paralela a través de CUDA 31 Big Data en el mundo del deporte 16 El poder de la inteligencia artificial 33 Desafíos y oportunidades del Cloud Computing en Guatemala 18 Voto electrónico 36 Es hora de regular el contenido de internet "],
["02-lmartinez.html", "Introduciendo Firebase Conociendo Firebase Más que una herramienta, un ambiente Como testimonio personal Conclusiones: Referencias bibliográficas:", " Introduciendo Firebase Luis Noé Martínez Rivera luis56009@gmail.com Estudiante de Ingeniería en Ciencias y Sistemas - USAC Palabras Clave: Tecnología, desarrollo, gestión, herramientas, servicios. Conociendo Firebase Te has preguntado alguna vez ¿Cuantas herramientas existen afuera que facilitan tu trabajo como desarrollador de software, que además poseen gran documentación para guiarte y son fáciles de utilizar? De las mejores respuestas que puedes obtener no podría faltar Firebase de Google. Pero ¿Qué es Firebase? No solo es una herramienta, es un conjunto de ellas que están orientadas a la creación o desarrollo de aplicaciones, todo esto bajo la calidad y experiencia de Google, que hará de este proceso más eficiente y que te enfoques más en tu idea. Todo esto suena muy bien, así que conozcamos a más detalle los servicios más destacados que podríamos utilizar “como se representa en la imagen 1”. Imagen 1 - Fuente: Firebase Google Uno de los principales servicios y por el cual es más conocido Firebase es su base de datos Realtime, con este servicio podrás almacenar cualquier cantidad de datos que requiera tu aplicación, es importante mencionarte que se almacenan con un formato Json (nombre: valor). Estas bases de datos podrán manejar reglas de acceso y de tipo de manejo de datos, ya sea solo de lectura o escritura o ambas. Siendo Firebase un conjunto de servicios en la nube, este maneja bases de datos NoSql, sin embargo Google le da el nombre de “Bases de Datos Realtime” para poder hacerlo más amistoso y darte la idea que los datos van y vienen sin que el flujo se detenga. Siguiendo con el tema de almacenamiento encontramos a Storage. Acá se nos brinda espacio para poder almacenar archivos, tanto de administración como de usuarios. Estos archivos siguiendo reglas de bases que podemos configurar. Es importante notar la diferencia entre almacenamiento de archivos y datos, uno alberga datos que mediante un estudio se convertirán en información, mientras que el otro, archivos digitales (imágenes, documentos, entre otros). Firebase también es muy conocido por brindar pequeñas soluciones de código a cualquier tipo de proyecto con el fin de volverlo más eficiente y amigable. Acá encontramos el manejo de Autenticación por ejemplo. Este servicio nos facilita el manejo del acceso de nuestros usuarios a la aplicación que estemos desarrollando. Simplifica el inicio de sesión y nos brinda la opción de poder ingresar con proveedores destacados, por ejemplo Google, Facebook, Twitter, GitHub, o Teléfono. De seguro has podido observar esto en otros programas y te has preguntado ¿Cómo se realiza esta funcionalidad? Ahora ya conoces uno de los lugares donde lo puedes encontrar. “Ver imagen 2”. Imagen 2 - Fuente: Firebase Google Otro gran ejemplo es Admob, este es uno de los más utilizados por las aplicaciones gratuitas de la red, nos permite colocar publicidad de manera amigable con el fin de mantener un ingreso capaz de financiar el desarrollo para programadores o aplicaciones principiantes. Dependiendo de la región donde se utilice tu aplicación Firebase manejara junto con Google los Analytics necesarios para tus usuarios. Ahora que has manejado funcionalidades para ti como desarrollador, te presentamos otros tipos de servicios que harán que la experiencia de usuario sea exitosa y amigable, lo cual atrae a más clientes potenciales. Uno de los más grandes problemas al desarrollar es el manejo de Notificaciones, no por su complejidad, sino porque con cada lenguaje debemos encontrar las herramientas específicas para mostrar de manera amigable mensajes que les brinden advertencias a los usuarios. Con Firebase esto deja de ser un problema como tal y pasa a ser un paso simple, podremos gestionar notificaciones con simples parámetros y Firebase se encargara del resto. Otro gran problema que se presenta con los usuarios es la interfaz ¿Cómo manejar de manera eficiente como se le presenta nuestra aplicación a los usuarios desde cualquier dispositivo? Con Enlaces dinámicos de Firebase nuestra aplicación se adaptaría a cada dispositivo, esto lo podemos gestionar mientras el desarrollo de nuestro programa se encuentra en marcha. Todo esto claro con la dirección y guía de Firebase, esto de manera estandarizada. Más que una herramienta, un ambiente Aunque Firebase sea más conocido como un conjunto de herramientas para desarrolladores también nos ofrece el servicio de Hosting, un ambiente de trabajo. Tal y como su nombre nos lo indica este servicio nos permite colocar nuestra aplicación bajo el cargo de Firebase y funcionalidades Cloud. Esto en una plataforma que nos ofrece seguridad, protocolos, alta escalabilidad y alta disponibilidad, todo lo que nos ofrecería otro servicio de la nube para montar nuestro programa. No está de más mencionar que cualquier tipo de software puede ser adquirir este servicio, ¡No existen limitantes! Ya sea que tu aplicación esté realizada con JavasScript, Python, aplicaciones de IOS, Android, entre otros, el SDK de Firebase siempre estará disponible. Otra gran ventaja de utilizar este servicio es la facilidad de desarrollar sitios estáticos o sitios prefabricados, es tan simple como instalar la herramienta de Firebase CLI, configurar tu proyecto dependiendo de tus necesidades e implementarlo. Tres simples pasos para soluciones eficientes. Todo esto también incluye la replicación del resguardo de tu información y aplicación alrededor del mundo “ver imagen 3”, dándole una latencia y consistencia de conexión de buena calidad desde cualquier punto. Imagen 3 - Fuente: Firebase Google Como testimonio personal Todos estos servicios y más al alcance de cualquier desarrollador en el planeta no pueden ser del todo gratuitos ¿verdad? Déjame decirte que algunos si lo son y dependerán de la magnitud del uso de los mismos. Firebase nos dejará experimentar y vivir la experiencia de utilizar muchas de sus herramientas de manera gratuita, solo deberemos formar parte de la familia de Google y listo. Dentro de Firebase podremos encontrar todo tipo de guías junto con tutoriales, todo lo necesario para que empieces a trabajar y desarrollar tus ideas. Para animarte aún más a probar esta grandiosa herramienta déjame contarte que en menos de un mes se logró construir un proyecto de IOT con hardware y software desde cero que incluía una base de datos NoSql, donde se almacenaba datos de manera masiva, software desarrollado en python, Arduino y Android, un circuito encapsulado y amigable que brindaba el control de tanques de almacenamiento de agua potable. Un proyecto totalmente viable sin invertir un solo dólar, gracias a los servicios que nos proporcionó Firebase. Así que no lo pienses más y únete a la comunidad y se parte del futuro de desarrollo de aplicaciones. Conclusiones: Firebase es un conjunto de herramientas que ayudan a desarrollar soluciones de alta calidad y eficiencia bajo los estándares y responsabilidad de Google. Los servicios de Firebase están guiados para que cualquier persona con conocimientos básicos de desarrollo de software pueda realizar infinidad de proyectos tecnológicos. Al ser servicios de Google estos nos brindan soporte, mantenimiento y guía de calidad en cualquier momento, así que los protocolos y aplicaciones agregadas que se utilicen serán de total confianza y totalmente eficientes. Referencias bibliográficas: Firebase google. Una plataforma integral para desarrollo de dispositivos móviles (04/2019) Firebase Google. Guía de Firebase (04/2019) Dennis Alund (02/2017) Gateway to Firebase: Hosting (04/2019) Miguh Ruiz (08/2017) ¿Qué es Firebase de Google? (04/2019) "],
["03-ealvarez.html", "Edge Computing Conclusiones: Referencias bibliográficas:", " Edge Computing Esteban David Alvarez Bor davix93a@gmail.com Estudiante de Ingeniería en Ciencias y Sistemas - USAC Palabras Clave: Cloud Computing, Edge Computing, AWS, Cloud, IoT, Latencia. Edge Computing es un concepto relativamente nuevo, surge a partir del concepto de Cloud Computing, en el cual se intenta acercar el almacenamiento y procesamiento a la fuente de los datos, para no enviar toda la data a un servidor remoto en la nube o a un sistema centralizado que procese la misma. Esto representa un cambio en la arquitectura provista por los sistemas tradicionales de Internet de las cosas, que utilizan Cloud Computing con la finalidad de proveer a sistemas la capacidad de procesamiento de datos, casi en tiempo real, mejorando así el nivel de respuesta de aquellos sistemas que su información es de suma importancia, como dispositivos médicos, industriales y muy recientemente en vehículos autónomos. IEEE define Internet de las cosas como “Un campo de aplicación que integra campos tecnológicos y sociales” . La firma de análisis Gartner nos proporciona una definición más extensa de la que podemos extraer \"IoT es una red dedicada de objetos físicos con tecnología integrada que mide y comunica la interacción entre factores internos y externos\" , además dentro de esta definición nos menciona un factor muy importante, como la data capturada permite a las compañías aprender el comportamiento y uso de sus sistemas, permitiéndoles tomar una serie de acciones preventivas o mejorar sus procesos de negocios. Si consideramos un sistema de Internet de las Cosas, desarrollado actualmente bajo el concepto de Cloud Computing, el cual Amazon Web Services define como “La entrega a pedido de poder de cómputo, almacenamiento, aplicaciones y recursos de TI a través de una plataforma en la nube en internet con precios basados en el consumo.”, hay dos factores clave que debemos considerar al momento de evaluar el grado de importancia como lo son la velocidad de procesamiento y el tiempo de respuesta. Aunque hay sistemas que se acoplan perfectamente a un entorno en la nube como en el caso de sistemas de domótica, hay otros como en el caso de sistemas de Internet de las Cosas del sector médico, bancario e industrial, que requieren de un grado superior de procesamiento y respuesta debido a la sensibilidad de la información que maneja, y factores como la conexión a internet, el lugar en donde se despliega el servidor, puede influir en los factores mencionados anteriormente. Para estos sistemas especiales, en los cuales la velocidad de procesamiento y respuesta son esenciales e indispensables, surge el concepto de Edge Computing o Cómputo en el borde, en el cual, su premisa es acercar tanto como sea posible el almacenamiento y procesamiento de la data al dispositivo que la genera, con el propósito de eliminar la latencia en estos procesos. Este concepto toma fuerza cuando analizamos la cantidad masiva de datos generada por un dispositivo de Internet de las Cosas, y el tiempo considerable que a esta le toma al ser enviada y analizada a través de la red, cuando decisiones como detener una línea de ensamblaje, reactivar una planta eléctrica o incluso diagnosticar mediante un monitor biométrico en el cual milisegundos pueden ser valiosos. Imagen 1 - Fuente: BI Intelligence Un estudio realizado por BI Intelligence pronosticó un estimado de 5.635 mil millones de dispositivos conectados bajo el modelo de Edge Computing para el año 2020 tanto en sectores del gobierno como de la industria. Para poder entender mejor el concepto de Edge Computing, se creó el concepto de Fog Computing o Cómputo en la niebla, donde podemos hacer la analogía con una nube física, en la cual la niebla es la parte exterior o borde de ella. Mas formalmente se define como “Una infraestructura de computo descentralizada en la cual tanto la data, cómputo, almacenamiento y aplicaciones están localizadas en un lugar intermedio de la fuente de datos y la nube.”. Este concepto define el estándar de como Edge Computing debe trabajar para facilitar las operaciones del modelo. Imagen 2 - Fuente: TechTarget En la actualidad, el ejemplo más acertado del uso de Edge Computing es en los sistemas utilizados por los vehículos autónomos. Estos vehículos, cuentan con un amplio conjunto de sensores los cuales recolectan grandes cantidades de información como velocidad, distancia, ruta, detección de objetos entre otros. Un artículo escrito por Kathy Winter, para Intel Newsroom, indica que se estima una cantidad de cuatro terabytes de datos recolectados por un vehículo autónomo en un solo día. Estos datos recolectados requieren de un rápido procesamiento y respuesta debido a que se espera una conducción lo más óptima y segura posible. Aquí entra Edge Computing agilizando el procesamiento de los datos generando resultados en tiempo real para un sistema complejo. Imagen 3 - Fuente: Intel Plataformas de Cloud Computing han desarrollado servicios capaces de dar solución a la problemática de latencia en despliegues totalmente en la nube, Amazon Web Services proporciona un servicio llamado Lambda@Edge, el cual tiene la finalidad de acercar la ejecución de código al usuario, mejorando el rendimiento y reduciendo la latencia, sin la necesidad de administrar infraestructura en distintas partes del mundo. IoT Greengrass es otro servicio proporcionado por AWS permitiendo una interacción a nivel local en función de los datos generados y el uso de la nube para tareas administrativas, análisis y almacenamiento duradero. Conclusiones: Edge Computing busca reducir la latencia acercando el procesamiento de los datos lo más cerca del lugar de donde se originan. Fog Computing define el estándar de como Edge Computing debería trabajar. El uso de Edge Computing depende de las características de nuestro sistema, la velocidad de procesamiento y velocidad de respuesta requerida para satisfacer las necesidades de este. Sistemas de información vital o de suma importancia son los principales en acoplarse al modelo de Edge Computing. Referencias bibliográficas: IEEE. (2015). towards a definition of the Internet of Things (IoT). 28/03/19, de IEEE Gartner. (2017). Leading the IoT. 28/03/19, de Gartner Amazon Web Services. (2019). What it Cloud Computing? 28/03/19, de AWS Margaret Rouse. (2019). Fog computing. 28/03/19, de IoT Agenda, TechTarget fogging) Amazon Web Services. (2019). Lambda@Edge. 28/03/19, de AWS Amazon Web Services. (2019). AWS IoT Greengrass. 28/03/19, de AWS David Linthicum. Edge computing vs. fog computing: Definitions and enterprise uses. 28/03/19, de Cisco Lauren Horwitz. Connected devices push business to the edge (edge computing architecture that is). 28/03/19, de Cisco Kathy Winter. (2017). for self-driving cars, there's big meaning behind one big number: 4 terabytes. 28/03/19, de Intel Newsroom "],
["04-fdepaz.html", "Flutter: el presente de las aplicaciones móviles Conclusiones: Referencias bibliográficas:", " Flutter: el presente de las aplicaciones móviles *Fabio César De Paz Vásquez** fabiodepazv@gmail.com Estudiante de Ingeniería en Ciencias y Sistemas - USAC Palabras Clave: Desarrollo de aplicaciones híbridas, Dart, Flutter, Google. En el contexto de aplicaciones móviles, tomar una decisión sobre qué grupo de herramientas utilizar para crear aplicaciones nuevas, es una decisión bastante compleja, ya que, al existir distintos sistemas operativos, se requiere que las aplicaciones puedan ejecutarse de manera correcta en cualquiera de estas. Como consecuencia a esto, algunas personas tienden a inclinarse por implementar sus nuevos proyectos utilizando lo que se conoce como aplicaciones móviles híbridas. Para poder explicar y adentrarse en esta pequeña parte del universo del desarrollo, es necesario presentar contexto respecto a esto. Imagen 1 - Fuente: Wikipedia, User: Senado Federal Para el desarrollo móvil, se cuenta con tres distintas alternativas a elegir, como se describen a continuación: Desarrollo nativo: A manera de simplificar su descripción, puede describirse como aquellas aplicaciones que son desarrolladas utilizando específicamente el lenguaje de la plataforma donde se ejecutan, por ejemplo: JAVA para Android o Swift para iOS. Esto podría complicar la situación de poder publicar la misma aplicación en distintas plataformas, ya que estas son excluyentes con los otros lenguajes nativos, lo que trae consigo el hecho de tener que realizar doble trabajo si se quisiera desarrollar para varias plataformas de manera nativa. Entonces, ¿por qué razones desarrollar de manera nativa? Permiten mejorar el performance de la aplicación. El look &amp; feel que se logra generar es óptimo y su fluidez es evidente. Es posible acceder a todos los componentes del dispositivo, como los sensores y actuadores. Estas razones parecen ser convincentes para que los desarrolladores opten por esta alternativa, pero si se quiere realizar implementaciones de manera masiva e invirtiendo menos recursos, esta termina siendo una opción compleja y costosa, debido a esto han surgido las siguientes formas de implementación. Desarrollo híbrido: Se trata de aplicaciones que se desarrollan implementando estándares de aplicaciones web, mediante HTML, CSS y JavaScript, que finalmente es renderizado en el teléfono mediante el uso de un framework específico (como lo puede ser PhoneGap, Ionic o Cordova). Esta parece ser una opción bastante viable, ya que realizando un único desarrollo es posible visualizar y ejecutar las aplicaciones tanto en ambientes web, como Android y iOS. Recientemente, este método es utilizado por muchas empresas y programadores a nivel mundial, ya que se ahorran el tiempo de desarrollo y reducen la complejidad del código, simplificándolo a una misma sintaxis y compilación. Estos frameworks ofrecen a los desarrolladores el acceso a funcionalidades nativas a través de ellos, pero como muchas cosas en el mundo, existen sus limitantes. Esto no quiere decir que sea de mala calidad, sino que de cierta forma no tienen acceso a todos los componentes del teléfono y además la fluidez visual no será la misma, debido a la existencia de una capa intermedia que se encarga de renderizar las vistas. Desarrollo generado o bridged: Se trata del desarrollo a través de distintos frameworks que ofrecen su sintaxis y características para escribir el código, pero finalmente estas se encargan de traducir estas instrucciones hacia instrucciones del lenguaje nativo de la plataforma sobre la cual se ejecutan. Teniendo esto en mente, es posible adentrarse un poco en lo que es Flutter2. Se trata de un framework de desarrollo de aplicaciones móviles creado por Google, teniendo su primer reléase al público en general en mayo de 2017, pero, presentando su primer reléase estable el 22 de febrero de 2019. Flutter se encuentra optimizado para crear aplicaciones tanto Android, como iOS y de manera novedosa, incluyen el sistema operativo en desarrollo propio de Google, Google Fuchsia. Imagen 2 - Fuente: Flutter Una de las características ofrecidas por Flutter, es que como código fuente utiliza el lenguaje Dart, que es un lenguaje de propósito general, también creado por Google y que se utiliza para construir aplicaciones en distintas plataformas, como web, de servidores, de escritorio e incluso aplicaciones móviles. Dentro de sus atributos es posible mencionar que se trata de un lenguaje orientado a objetos, que utiliza definición por clases, cuenta con su propio Garbage Collector y su sintaxis es similar al lenguaje C. Para describir de mejor manera Flutter, es necesario describir sus componentes: Flutter engine: Es el core de Flutter, se encuentra desarrollado en C++ y proporciona soporte bastante amplio a bajo nivel del dispositivo, lo que permite el acceso al hardware del dispositivo. Se vincula de manera directa con los SDK de Android y de iOS. Foundation library: Son componentes escritos en Dart, que pone a disposición de los programadores, para que puedan reutilizarlo, se trata de clases y funciones básicas que son requeridas para ejecutar Flutter, un ejemplo de esto serían las APIs utilizadas para hacer llamadas a su engine. Widgets: El desarrollo de la interfaz de Flutter se basa en widgets. Cada componente visual que se puede apreciar es un widget. Dentro de las ventajas de Flutter, está en que es posible combinar distintos widgets con el objetivo de generar nuevas combinaciones, tal como las indique o requiera el programador. Dentro de los widgets se ofrece de manera original, los diseños visuales de Material Design (usado por Android) y de Cupertino (diseños usados por iOS). Una vez descrito qué es Flutter y cómo es su arquitectura principal, el lector podría pensar: ¿por qué usar Flutter? ¿Por qué no simplemente utilizar algún otro framework de renderización web?1. Pues existen muchas respuestas a estas preguntas, tales como el soporte de la comunidad, el auge de este nuevo framework, lo cual se puede notar en su alta demanda3 mediante la imagen 3. Otro motivo, es el respaldo que ofrece Google, una empresa de gran trayectoria y actualización constante, quienes además brindan soporte técnico, implementación de nuevas características, y la frecuente reparación de posibles bugs. Todo esto suena bien, pero va un poco por encima de lo que es en realidad la ventaja principal de Flutter, por lo que se requiere una especificación más técnica: Flutter no utiliza un render de tipo web, en cambio, utiliza un canvas optimizado, en el que renderiza sus propios widgets con llamadas al sistema a bajo nivel, esto mediante la utilización de Dart. Esto permite tener acceso a la mayor cantidad de hardware del dispositivo (sensores, cámaras, actuadores), teniendo una renderización visual casi natural que es compatible con los sistemas operativos antes mencionados. Todas estas características dejan a Flutter como un fuerte candidato a considerar, sin mencionar que ya está siendo implementado por Google, Google ADs, Tencent, y el gigante de ventas mundiales Alibaba, quien recientemente implementó Flutter para su aplicación móvil, y que cuenta con más de 50 millones de descargas. Imagen 3 - Fuente: Google Conclusiones: El hecho de decidir entre qué tipo de implementación realizar, ya sea nativa, híbrida o generada, es una decisión importante que deben tomar los desarrolladores y encargados de sistemas, debiendo evaluar sus características ofrecidas y las necesidades que deben cubrir. Flutter es un framework de desarrollo potente, de gran auge que permite a los programadores, simplificar el tiempo y complejidad del proceso de escritura de código. La forma de generar aplicaciones a través de Flutter es lo más similar a una aplicación nativa, guardando los accesos a hardware y la fluidez de las interfaces gráficas, permitiendo además la generación de aplicaciones multiplataforma en un único código fuente. Referencias bibliográficas: Guillermo Martínez. Desarrollo de aplicaciones móviles híbridas ¿Me conviene?(12/09/2017) Marco Bellinaso. Flutter: the good, the bad and the ugly (23/11/2018) Flutter Team. Flutter Release Preview 2: Pixel-Perfect on iOS (19/09/2019) "],
["05-etejaxun.html", "Computación paralela a través de CUDA Computación paralela ¿Qué es CUDA? ¿Por qué CUDA? Paralelizar una aplicación a través de CUDA Resultados Conclusiones Referencias bibliográficas:", " Computación paralela a través de CUDA Erick Roberto Tejaxún Xicón erickteja@gmail.com Estudiante de Ingeniería en Ciencias y Sistemas - USAC Palabras Clave: CUDA, Kernel, Paralelismo, Multicore, Instrucción, GPU. Computación paralela La computación paralela hace referencia a la capacidad de poder realizar varias tareas al mismo tiempo. Esto aplicado a las ciencias de la computación se ha estado trabajando desde hace muchos años, casi al mismo tiempo que el inicio de la computación moderna. Pero, ¿para qué aplicar computación paralela? La computación paralela tiene como objetivo acelerar una aplicación, es decir, reducir el tiempo de procesamiento. Por lo cual el motivo que ha traído consigo la búsqueda de un estándar para el uso de computación paralela ha sido ese, de tener problemas que requieren gran cantidad de capacidad y tiempo de cómputo. Por ejemplo, un problema de simulación de colisiones de partículas, utilizando un algoritmo secuencial y usando un solo procesador podría llevar años en su finalización. En un principio, cuando se contaba con una computadora con un único procesador, el paralelismo se aplicó creando una red de computadoras dentro de las cuales se pudiera trabajar en un mismo problema. Es decir, cada computadora conectada a esta red, tendría la tarea de trabajar sobre una porción del problema. Minimizando así el tiempo de computación para la solución del problema. Esto se sigue aplicando hoy en día y podemos encontrar sitios especializados en este tema, que llevan el recuento y el listado de las supercomputadoras más potentes del mundo como top500.org. Imagen 1 - Fuente: Top500.org Como vemos en la ilustración anterior tomada de la página Top500.org, las supercomputadoras en la actualidad cuentan con millones de núcleos que pueden trabajar en conjunto y generar un throughput en el orden de miles de Teraflops por segundo. Cabe mencionar que estas supercomputadoras están en centros de investigación en países del primer mundo. A través del tiempo, la computación paralela ha sido un tema estudiado de sobremanera ya que tiene muchas aplicaciones en cualquier ámbito. Quizá en algún futuro se pueda contar en nuestra escuela con un curso sobre el tema como el que se cuenta ya en la Escuela de Ciencias Físicas y Matemáticas el cual tiene un enfoque científico donde se tratan de resolver problemas de sistemas dinámicos, corridas de método Montecarlo entre otros. Se han diseñado diferentes arquitecturas de memoria para la computación paralela. Esto debido a que el principio de esta es que se debe de trabajar sobre un mismo conjunto de datos que conforman todo el problema en sí. Entre estas arquitecturas tenemos la arquitectura de acceso uniforme a memoria (UMA por sus siglas en inglés), arquitectura de acceso no uniforme a memoria (NUMA por sus siglas en inglés). Dentro del paralelismo a nivel de datos existen cuatro técnicas básicas las cuales son: única instrucción, múltiple datos (SIMD siglas en inglés). Múltiple instrucciones, múltiple datos (MIMD), única instrucción, un dato (SISD) y múltiple instrucción, un dato (MISD). La más utilizada es la de única instrucción, múltiples datos. ¿Qué es CUDA? CUDA (Arquitectura unificada de dispositivos de cómputo) es una arquitectura de procesamiento en paralelo creada por la empresa especializada en tarjetas gráficas Nvidia, en la que se trata de aprovechar la capacidad y potencia de sus GPU (Unidad de procesamiento gráfico) como una alternativa al procesamiento en CPU’s tradicionales. CUDA es una tecnología basada tanto en hardware como en software. En hardware al hacer que la arquitectura de sus tarjetas gráficas estén preparadas para poder explotar el paralelismo al contar con varios conjuntos de procesadores que reciben una misma instrucción para un vasto conjunto de datos. Entre ellas encontramos las arquitecturas Maxwell, Pascal entre otras. En cuanto a software, CUDA provee una plataforma en forma de librerías y compiladores para que se puedan escribir programas que aprovechen el hardware de las GPUs dándole total control al desarrollador. ¿Por qué CUDA? Existen diferentes arquitecturas de computación paralela. Estás tienen su máximos exponentes en especificaciones y estándares tan potentes y utilizados como OpenMP2 o bien MPI los cuales en la actualidad son los que suelen ser utilizados en la mayoría de centros de investigación. Lo que trata de hacer CUDA es desviarse de lo que buscan los procesadores tradicionales como los de la serie Xenon de Intel, en donde sus núcleos son muchísimo más rápidos que un núcleo de una GPU estándar (hasta 4.5 Ghz vs 1096 – 1020 Mhz) en vez de enfocarse en la rapidez de los núcleos, se enfoca en la cantidad de estos núcleos. Así, en vez de tener 24 o 36 núcleos a una velocidad de 4.5Ghz, una GPU cuenta con 640, 1020 y hasta 1280 núcleo (en las tarjetas de video serie titán) con una frecuencia de reloj de 1078Mhz. Esto permite aprovechar la técnica SIMD de simple instrucción para múltiple data. Por esa razón, CUDA se acopla demasiado bien para aplicar computación paralela y así maximizar el rendimiento de las aplicaciones. Cabe mencionar que CUDA es transparente al sistema operativo, es decir que CUDA ofrece soporte para sistemas operativos Windows, GNU/Linux y para sistemas operativos MacOs. Su rendimiento no depende del sistema operativo en sí, si no en las condiciones de trabajo que se encuentre la máquina o clúster sobre el cual se hará funcionar la aplicación. Paralelizar una aplicación a través de CUDA Para comprobar, analizar y determinar el mejoramiento real de usar computación paralela en un problema real (hay que tener en cuenta que no todos los problemas se pueden paralelizar ya que por lo general son inherentemente secuenciales) se utilizarán una aplicación llamada ScanSky, desarrollada y escrita por la Dra. Ana Moretón Fernández, el Dr. Javier Fresno y el Dr. Arturo González-Escribano del grupo de investigación Trasgo3, de la escuela superior de ingeniería informática de la Universidad de Valladolid, España, como parte de la práctica número tres del curso de computación paralela. Esta aplicación recibe como entrada un archivo que consiste en números separados por un salto de línea tal y como se muestra en la siguiente imagen. Estos números representan el color de cada uno de los píxeles de una imagen. El objetivo de esta aplicación es determinar con precisión el número de cuerpos celestes presentes en dicha imagen. Imagen 2 - Fuente: Elaboración propia. Imagen 3 - Fuente: Nasa Imagen 4 - Fuente: Elaboración propia. El programa determina el número de cuerpos celestes compara un pixel con los pixeles de sus alrededores para determinar si se trata o no del mismo cuerpo celeste, comenzando de izquierda a derecha y de arriba hacia abajo, es decir empezando con la coordenada (0,0) hasta llegar a la coordenada (n,n) . El código del programa secuencial, escrito en lenguaje de programación C, se puede encontrar en github 3, subido con los permisos pertinentes de los autores. Y este cuenta con el programa secuencial que cuenta los planetas en el archivo de entrada, apoyado con unas librerías desarrolladas por el mismo grupo de investigación que permite obtener el tiempo exacto de ejecución y otra librería que permite conectarse hacia el sistema Tablón 5 que permitía correr el sistema dentro del clúster de la escuela superior de ingeniería informática. Para esta prueba, vamos a comparar los tiempos de ejecución en diferentes condiciones obedeciendo las siguientes especificaciones: Imagen 5 - Fuente: Especificaciones corrida secuencial de ScanSky - Elaboración propia. Imagen 6 - Fuente: Especificaciones corrida paralela de ScanSky a través de CUDA - Elaboración propia. Imagen 7 - Fuente: Especificaciones corrida paralela ScanSky a través de CUDA. Sistema Tablón, UVa - Elaboración propia. Para estas corridas de prueba se utilizaron 3 tipos de cargas de datos como se indica a continuación: Imagen 8 - Fuente: Set de datos para pruebas con el programa ScanSky - Elaboración propia. Resultados Imagen 9 - Fuente: Elaboración propia. Imagen 10 - Fuente: Elaboración propia. Imagen 11 - Fuente: Elaboración propia. Como podemos observar en la gráfica anterior, las tres configuraciones tienen una forma similar, pero con una pendiente mucho más pequeña en el caso de la configuración número 3. Y vemos que utilizando la GPU de la computadora de la configuración número 1, obtenemos un resultado que minimiza sobre 3 veces el tiempo utilizado en la aplicación en forma secuencial. Obviamente el código en paralelo utilizando CUDA es mejorable, pero en principio vemos como la computación paralela puede ayudarnos a reducir el tiempo de procesamiento en set de datos pequeños, pero cuando se utilicen set de datos muchos mayores, la diferencia será demasiado grande. Conclusiones La computación paralela es un tema muy importante en el área de las ciencias de computación y además en muchas industrias como la de la medicina, ingeniería, química, física, aviación, videojuegos entre muchas otras. Vemos como en un programa no tan complejo, la programación paralela nos ha ayudado a realizar una aceleración de hasta tres veces menor con un set de datos mediano, pero con set de datos mayores, la aceleración será mucho mayor a comparación de su versión secuencial. Es necesario que como escuela de ciencias y sistemas estemos anuentes a estas tendencias tecnológicas que no son recientes pero que tienen mucho auge y demanda en muchas industrias y que podría ser otra ventana de salida para los futuros egresados de nuestra escuela de ciencias y sistemas. Referencias bibliográficas: Programa del curso de física computacional, ECFM, USAC. Disponible en (1) Especificación para computación paralela por medio de paso de mensajes (API). OpenMP (2)) Grupo de investigación, Trasgo. Universidad de Valladolid, España. (3) Programa ScanSky paralelo y su conversión a su equivalente en versión paralela a través de CUDA. (4) Dr. Javier Fresno, Sistema tablón (5)) Blaise Barney, Lawrence . National Laboratory, “Introduction to parallel computing” En línea consultado el 05/04/2019. WOMPAT 2001, International Worskshop on OpenMP Aplication, “OpenMp Shared Memory Parallel Programing”, Purdue University, School of Electrical and Computer Engineering. Nvidia, Cuda Toolkit documentation. En línea Consultado el 01/04/2019. Michael Skuhersky, MIT. “Introduction to parallel computing”,En línea consultado el 03/04/2019. "],
["06-agramajo.html", "Optimización de sitios web con Google Analytics Conclusiones: Referencias bibliográficas:", " Optimización de sitios web con Google Analytics Anibal Vinicio Gramajo Ramirez gramajo.anibalv@gmail.com Estudiante de Ingeniería en Ciencias y Sistemas - USAC Marketing, SEO, Analytics, análisis, campaña publicitaria. Una de las principales utilidades de internet es la posibilidad de dar a conocer nuestros productos y servicios a nuevos clientes, a través de diferentes canales de comunicación, tales como redes sociales, foros de debate, motores de búsqueda, entre otros. Debido a la creciente popularidad y extensión de los servicios de telefonía móvil e internet, puede tornarse difícil asegurarse que el mensaje correcto llegue a la persona correcta. Es decir, que nuestros productos y servicios sean vistos por aquellas personas que tengan más probabilidad de adquirirlos o, en otras palabras, segmentar el mercado. Debido a esta necesidad nacen nuevos y diferentes productos que nos ofrecen empresas como Google, estas creadas con el objetivo de sacar el mayor provecho a nuestras campañas publicitarias y así mejorar las ventas de nuestro negocio. Imagen 1 - Fuente: Prestashop.com Como estudiantes de Ingeniería en Sistemas, muchas veces ignoramos el alcance que pueden llegar a tener los sistemas que podemos llegar a producir. Sabemos que hoy en día, es prácticamente indispensable para cualquier negocio tener una computadora. Ya sea para llevar control de sus ventas, contactos, recordatorios o simplemente conectarse a internet, se vuelve una necesidad tener un sistema automatizado que permita a las empresas librarse de los viejos libros de papel. Una de las necesidades que siempre han tenido las empresas es darse a conocer, así como los servicios y productos que ofrecen a posibles nuevos clientes siendo la web el canal más utilizado en esta nueva era. Sin embargo, en el inmenso mundo web es muy difícil lograr que las personas indicadas vean exactamente lo que queremos. Por ejemplo, ¿Cómo logramos que nuestro producto sea visto por determinado grupo de personas? ¿Cómo sabemos cuál anuncio es más eficiente que otro? ¿Cómo sabemos el rango de edades de las personas que nos visitan? Estas y muchas interrogantes más tienen respuesta debido a diversas herramientas que los navegadores y buscadores ponen a nuestra disposición, entre ellas Google. Siendo estudiante de la carrera de Ingeniería en Sistemas desconocía la estrecha relación que nuestra profesión tendría con otras tan diversas como la contabilidad, publicidad, recursos humanos, etc. Fue hasta que empecé a laborar como freelancer para una empresa estadounidense de marketing que llegué a conocer el impacto que tiene el uso correcto de las campañas publicitarias en línea para la atracción de nuevos clientes a los negocios. Una de las herramientas que llamó de primero mi atención fue Google Analytics1 la cual nos permite conocer más sobre el origen y el comportamiento de los visitantes (posibles clientes) de nuestro sitio web y de esta manera brindarle la información que le interese a fin de captar su atención y hacer, que esa visita se concrete en una venta. Esto es posible gracias a que Google Analytics realiza la inserción de código en los hipervínculos, tanto de nuestro sitio web como de los anuncios de nuestra campaña publicitaria que permite obtener un perfil bastante completo del usuario. Por medio de este código, llamado “Etiquetas” (Tags en inglés) podemos obtener características detalladas de nuestros visitantes. Entre las principales se encuentran las siguientes: Edad, ubicación geográfica, idioma, número de páginas de nuestro sitio visitadas, tiempo de duración de la sesión, visitantes de retorno, dispositivo utilizado para la visita, etc. Imagen 2 - Fuente: Simplus.com Conocer esta información nos ayudará a mejorar la experiencia que el usuario tendrá al visitar nuestro sitio web y podremos ofrecerle contenido específico para su perfil. ¿Estoy atrayendo la audiencia indicada? ¿Están permaneciendo en mi sitio web mis visitantes? Si no, ¿Por qué no? ¿Qué debo cambiar? ¿Les atraen mis ofertas? Como podemos ver, gracias al análisis del usuario que podemos realizar con esta herramienta podemos personalizar nuestro sitio web a fin de satisfacer sus necesidades y aprovechar para hacernos de un nuevo cliente. Podemos, por ejemplo, ofrecer un cupón de descuento a un usuario indeciso que sabemos, ya ha visitado nuestro sitio web con anterioridad sin concretar ninguna compra. Todo esto se logra por medio de las etiquetas que Google Analytics utiliza. Una de las ventajas de esta herramienta es que posee una poderosa interfaz en línea a la cual podemos acceder desde el sitio web de Google (llamada “Google Console”) que nos permite configurar las etiquetas que se incrustarán en nuestros enlaces y con ello decidir los aspectos que queremos recopilar de nuestros usuarios. Además, también nos provee de los resultados obtenidos y análisis estadísticos de los visitantes de nuestro sitio. Cabe destacar que el uso de esta herramienta junto con Google Adwords2 nos permite crear poderosas campañas publicitarias para una población específica y anuncios que atraigan más visitantes a nuestro sitio web, los cuales obtendrás los mejores productos de acuerdo al perfil que Google Analytics nos permite obtener. Existe una gran variedad de utilidades que podemos obtener de estas dos herramientas combinadas y con ello, podemos lograr a comprender de una mejor manera a nuestros visitantes, su comportamiento e intereses. Es por ello que Google nos ofrece los programas de capacitación3 y certificación en estas herramientas completamente gratis. Esto es una gran ventaja ya que puedes agregar a tu resumen de vida certificaciones4 avaladas por una empresa muy reconocida a nivel mundial como lo es Google. Conclusiones: Al analizar el origen, comportamiento y perfil de los visitantes de nuestro sitio web podemos obtener mejores resultados en nuestras campañas publicitarias al mejorar su experiencia en nuestro sitio y ofrecerle solo productos de su interés. El comportamiento de nuestros nuevos visitantes en nuestras páginas de aterrizaje (landing pages) nos permitirá también medir el nivel de efectividad en la manera de promocionar u ofrecer nuestros productos. De esta manera podemos obtener un mayor beneficio de los usuarios que ya se han mostrado interesados. Existen diversos medios de capacitarse en herramientas especializadas de diferentes empresas, tales como Google, Yahoo, Microsoft, etc. Muchas de ellas gratuitas que nos permitirán ampliar nuestro conocimiento y agrandar nuestro resumen de vida. Referencias bibliográficas: Google Analytics (1) Google Ads (2) Google Developers (3) Google Support (4) "],
["07-dmomotic.html", "El poder de la inteligencia artificial Conclusiones: Referencias bibliográficas:", " El poder de la inteligencia artificial Diego Antonio Momotic Montesdeoca diegomomotic@gmail.com Estudiante de Ingeniería en Ciencias y Sistemas - USAC Palabras Clave: Inteligencia artificial, redes neuronales, historia, guerra, comparativa, enigma. Durante el desarrollo de la segunda guerra mundial en los años 1939-1945, fueron llevadas a cabo grandes hazañas de distinta índole, una de estas proezas fue realizada en Bletchley Park Inglaterra, donde la inteligencia británica se encargó de reunir a un grupo de matemáticos, criptógrafos, ajedrecistas y científicos, considerados como los grandes genios de aquella época, con el claro objetivo de descifrar los mensajes codificados de los alemanes, que día a día eran interceptados. ¿Cuál era la dificultad? Resulta que, los alemanes utilizaban una máquina llamada “Enigma” para encriptar sus comunicaciones, este era un dispositivo electromecánico con apariencia similar a una máquina de escribir, como se puede apreciar en la imagen 1. Imagen 1 - Fuente: Neoteo.com La máquina contaba con ciertas configuraciones que se utilizaban para codificar los mensajes de distinta forma todos los días, es decir, las claves utilizadas para cifrar y descifrar los mensajes cambiaban continuamente, lo que añadía mayor complejidad a la situación. Para que puedas darte una idea la máquina tenía 3.283.883.513.796.974.198.700.882.069.882.752.878.379.955.261.095.623.685.444.05 posibilidades distintas de codificación1. ¿Un número gigantesco no? A simple vista puede considerarse una tarea prácticamente imposible, sin embargo, te sorprenderá saber que, gracias al esfuerzo y genialidad del grupo de personas reunidas en Bletchley Park, liderados por una de las mentes más brillantes de la historia, el matemático, científico y criptógrafo que en la actualidad es considerado uno de los padres de la computación e inteligencia artificial, Alan Turing, el código de la máquina enigma pudo ser descifrado luego de varias modificaciones a finales del año 19422. Algunos historiadores afirman que el logro sin precedencias de Turing y su equipo de trabajo, acortó la guerra 2 años y salvó aproximadamente 14,000 vidas. Es una historia impresionante sin duda, pero ¿Qué relación tiene con la Inteligencia Artificial? Antes de responder a esta pregunta, me gustaría definir los siguientes conceptos: Inteligencia artificial: es la simulación de procesos de inteligencia humana por parte de máquinas, especialmente sistemas informáticos. Estos procesos incluyen el aprendizaje, el razonamiento y la autocorrección4. Redes neuronales: es un modelo que intenta imitar el funcionamiento del cerebro humano, a través del impulso de entradas y salidas, dejando que la máquina decida el proceso adecuado para transformar las entradas en las salidas solicitadas. Ahora sí, teniendo en mente estos conceptos ¿Considerarías que la inteligencia artificial es capaz de resolver el código enigma?, si tu respuesta es afirmativa ¿Cuánto tiempo crees que tardaría? Durante el año 2017, según Rafal Janczyk co-fundador de la empresa Enigma Pattern, decidieron replicar el proyecto para descifrar el código de la maquina enigma, pero esta vez utilizarían el poder de la inteligencia artificial para lograrlo. Este proyecto nació, al formularse la pregunta ¿Qué hubiese sido capaz de hacer Alan Turing si hubiese tenido el poder de cómputo actual y el desarrollo que tenemos ahora de la inteligencia artificial? 5. Ahora que conoces la historia y tienes una idea de la complejidad, dificultad y cantidad años de trabajo que conllevó, a las grandes mentes de aquel entonces, el resultado obtenido por el grupo de genios en Bletchley Park es más que sorprendente. Es acá donde radica nuestra verdadera comprensión del poder de procesamiento y capacidad de la inteligencia artificial, ya que con el uso de 2,000 servidores de DigitalOcean (empresa estadounidense que provee servicios en la nube), al trabajar con redes neuronales se logró probar alrededor de 41 millones de contraseñas por segundo y en tan solo 13 minutos la inteligencia artificial fue capaz de descifrar el código. ¿Increíble no?, resulta que aún hay más, el costo real del uso de los 2,000 servidores durante este periodo de tiempo, según DigitalOcean, fue únicamente de 7 dólares6. Te imaginas el tiempo que se habría acortado la guerra y la cantidad de vidas salvadas, si este tipo de tecnología hubiese existido durante la segunda guerra mundial, sin duda la historia sería muy diferente. Es muy probable que hoy en día no utilicemos este tipo de herramientas para descifrar códigos en medio de guerras, sin embargo, podemos sacar provecho de ellas en distintas áreas como la robótica, finanzas, educación, entretenimiento, etc. En resumen, no tenemos restricción alguna para su aplicación, más que nuestra imaginación. Conclusiones: El uso de la Inteligencia artificial es tan útil hoy en día como lo hubiese sido décadas atrás. Dotar a los sistemas tecnológicos e informáticos con comportamientos similares a la “inteligencia humana”, nos ha encaminado, hacia lo que muchos consideran la 4ta revolución industrial. Sin darnos cuenta, día con día interactuamos con sistemas que ya se encuentran dotados de inteligencia artificial, como cuando vemos un video en YouTube (que utiliza redes neuronales), o al realizar una compra en el portal de Amazon, entre muchos otros. El desarrollo de los proveedores de servicios en la nube como: DigitalOcean, Google Cloud, Azure, Amazon Web Services, etc. Han puesto a nuestro alcance un poder de procesamiento de datos, que hace algunos años atrás no podíamos ni imaginar. Referencias bibliográficas: JJ Velasco (12/07/2011) La máquina Enigma, el sistema de cifrado que puso en jaque a Europa. (27/03/2019) (1) Pablo G. Bejerano (06/02/2014). Código Enigma, descifrado: el papel de Turing en la Segunda Guerra Mundial. (27/03/2019) (2) Mary Villarroel Sneshko (07/06/2018). La historia del genio que logró descifrar el Código Enigma. (27/03/2019) (3) Margaret Rouse (abril/2017). Inteligencia artificial, o AI. (27/03/2019) (4) Manuel López Michelone (09/07/2018). Descifrando el código de la máquina Enigma con Inteligencia Artificial (28/03/2019) (5)(6) "],
["08-dajuchan.html", "Voto Electrónico Registro Electrónico directo –DRE- Reconocimiento Óptico de Marcas–OMR- Impresora de papeletas electrónicas – EBP- Conclusiones: Referencias bibliográficas:", " Voto Electrónico Daniel Orlando Ajuchán Yancis yancisdl@gmail.com Estudiante de Ingeniería en Ciencias y Sistemas - USAC Palabras Clave: Voto electrónico, Sistemas de votación, internet, Voto no presencial. El voto electrónico es visto como una modalidad en la cual se hace uso de sistemas electrónico-digitales en parte o en todo el proceso de votación para acelerar el procesamiento de los resultados, proporcionar asistencia tecnológica y en general, aportar eficiencia en los procesos de elección a cargos populares. Sin embargo, esta definición puede ser demasiado amplia, por lo que es necesario presentar los diferentes sistemas de votación electrónicos conocidos o más implementados en la actualidad. Imagen 1 - Fuente: Mic Registro Electrónico directo –DRE- En estos sistemas, los electores emiten su voto utilizando un hardware que puede tener pantalla táctil, botones físicos, etc. Cada uno de estos aparatos cuenta con un disco duro para almacenar el conteo de votos de manera local. Al terminar el evento de votación, los resultados parciales de cada máquina se envían a un servidor para obtener los resultados finales. El problema con este sistema es que no existe una forma de garantizar que el voto emitido se haya registrado de manera correcta, o se haya contabilizado una sola vez debido a que este procedimiento se realiza de forma oculta a los usuarios. Para evitar el problema del sistema anterior y garantizar que el registro y conteo de votos se realizó de forma correcta, surge la variante que implementa la impresión de una papeleta o comprobante de auditoría de papel verificado por el votante -VVPAT-. En esta variante cada uno de los aparatos además de tener la pantalla táctil cuentan con una impresora, que luego de registrar cada voto, imprimen la boleta con la información necesaria que se mostrará al usuario para asegurar que el voto se registró de manera correcta. Algo importante en este sistema es que la boleta impresa no se les entrega a los usuarios, únicamente se muestra (regularmente detrás de un cristal), para luego ser depositada en una urna. Utilizando esta modalidad, se puede llevar un conteo interno en cada aparato o se puede contar únicamente las boletas de cada urna. De esta manera el proceso se hace auditable y se evitan posibles fraudes que pueden darse en sistemas sin VVPAT Reconocimiento Óptico de Marcas–OMR- En estos sistemas se utilizan papeletas especiales en la que la opción elegida por el votante es reconocida por lectores ópticos. (OMR, OCR, ICR). Hay que notar que esta modalidad en realidad representa una asistencia en el proceso de conteo, ya que únicamente reconocen marcas en las boletas para contabilizarlas. Existen dos formas de utilizar los lectores de reconocimientos ópticos de marcas, se puede disponer un lector en cada mesa para realizar el conteo cuando el votante introduce su voto en la urna, en este caso se lleva varios conteos parciales. Otra forma es utilizando este método es realizar el conteo de papeletas de forma centralizada pasando todas por un lector de este tipo. Impresora de papeletas electrónicas – EBP- En este sistema, el votante elige su candidato utilizando un aparato muy similar al utilizado en el sistema DRE, y luego de registrar el voto, este aparato imprime una boleta con marcas especiales para que pueda ser leída por un sistema óptico. De esta manera se separa el proceso de emisión de votos y el conteo de los mismos. También genera ayuda a generar confianza, ya que todos los electores pueden verificar su voto en las papeletas antes de depositarlo en las urnas donde serán contabilizados por el sistema óptico. Otro de los beneficios de este sistema es que las boletas serán leídas con un gran porcentaje de confianza, a diferencia de cuando se utilizan boletas marcadas por los usuarios, en donde regularmente los sistemas ópticos no reconocen las marcas de forma correcta. Esta modalidad es considerada una de las seguras, aunque por todo el hardware necesario, también es uno de los métodos más costosos de implementar, no solo en términos monetarios, también es necesario una capacitación adecuada para los usuarios. Los sistemas descritos anteriormente se caracterizan porque los electores deben emitir su voto de forma presencial, es decir, deben asistir personalmente a los centros de votación autorizados, en donde se realiza manualmente la identificación de cada votante y luego son autorizados a emitir su sufragio, por lo tanto, no hay manera de relacionar el voto con el votante, lo cual se traduce en confianza por parte del elector ya que se mantiene el secreto del voto. Existe también sistemas no presenciales, en los cuales no se necesita asistir a un lugar en particular, tampoco es necesario hardware especializado como en las modalidades anteriores, en estos casos la autenticación del votante y la emisión del sufragio se realiza en el mismo dispositivo (computadora, tablet, teléfono inteligente), usualmente se realizan a través de internet y siempre son independientes del dispositivo. En estos sistemas, los votos se registran conforme se van emitiendo ya que son transmitidos hacia un servidor, en el cual se tiene un conteo en tiempo real. Si bien este sistema ofrece resultados en tiempo real, también es uno de los más susceptibles a ataques tanto internos (manipulación del software durante su desarrollo) y externos (ataques a los servidores, manipulación de datos durante la transmisión, etc.). Imagen 2 - Fuente: Voto electrónico Al realizar la autenticación del votante y la emisión del voto en el mismo dispositivo, se corre el riesgo de que se pierda el carácter secreto del voto. Este sistema en particular, presenta muchas debilidades, aunque también ofrece beneficios que no se pueden conseguir con ninguno de los sistemas presenciales, por ejemplo: la accesibilidad ya que no es necesario reunirse en un lugar específico, lo cual beneficia en gran medida a personas con algún impedimento para transportarse de un lugar a otro; también provee rapidez y facilidad de uso ya que la mayoría de personas están familiarizados con dispositivos inteligentes. En general, provee una reducción significativa de costos. Una de las características más importantes en un sistema de elección a cargos populares, es la transparencia desde antes de iniciar el proceso, hasta el conteo y publicación de los resultados finales, los sistemas de voto por internet y DRE (sin VVPAT) carecen de esta cualidad, en caso de éstos últimos, también presentan el inconveniente de que no todas las personas comprenden el uso de ese tipo de tecnología, esto hace que muchas personas en edad adulta desconfía de estos sistemas y más aún, en áreas rurales en donde el índice de analfabetismo es muy alto. Independientemente del sistema que se implemente, se debe garantizar que se cumplan las características principales del voto: unicidad, es decir, que cada persona pueda votar una sola vez; el voto debe ser secreto, en otras palabras, que no haya ninguna forma de relacionar al votante con el voto y, por último, que únicamente las personas registradas en el padrón electoral puedan emitir su voto. Imagen 3 - Fuente: Voto electrónico Conclusiones: La implementación de sistemas electrónicos y digitales en los procesos de votación ofrecen ventajas frente al sistema tradicional de voto utilizando papeletas impresas, por ejemplo: se obtienen resultados casi instantáneos al finalizar el evento. También proporciona mayor accesibilidad, en cuanto a idiomas, el hardware y software puede ser configurado para mostrar la información en distintos idiomas según sea necesario. Los sistemas electrónicos presenciales ofrecen una mayor ventaja en cuanto a prevención de fraudes comparado con los que se hacen por medio de internet. Referencias bibliográficas: Luis Panizo Alonso (12/2017). Aspectos tecnológicos del voto electrónico. (02/04/2019). Patricia pescado, Ariel Pasini. El voto electrónico sobre internet. (02/04/2019) "],
["09-lazurdia.html", "El conocimiento no se destruye se transforma Conclusiones: Referencias bibliográficas:", " El conocimiento no se destruye se transforma Luis Estuardo Azurdia Cárcamo luis.azurdia.carcamo@gmail.com Estudiante de Ingeniería en Ciencias y Sistemas - USAC Palabras Clave: Educación, Nube, ITIL, Sistemas Operativos, Redes de Computadoras El mundo de la tecnología siempre está en constante cambio, las personas que trabajan en IT deben estar junto al cambio de la tecnología, al igual que centros educativos que brindan conocimiento a personas que buscan capacitarse como tal. Ninguna persona logrará saberlo todo, pero existe un conocimiento general que es la base de todo lo que se ha construido ahora. Últimamente el auge de la nube ha solucionado gran cantidad de problemas, dando así la versatilidad de obtener recursos a bajo precio y de forma casi inmediata. Las personas con conocimientos generales, tales como redes, sistemas operativos compiladores entre otros, pueden crear una gran infraestructura por medio de un par de clics con el paso de tiempo resulta más simple poder llevar a la práctica los conocimientos adquiridos en talleres o en alguna clase de programación. La experiencia adquirida al utilizar la nube es grandiosa, solamente es necesario tener el conocimiento de lo que desea hacer, ya que casi todo está enlatado en algún servicio que brinda cada proveedor. Los proveedores de la nube ofrecen servicios de seguridad externa para la infraestructura tales como un servicio de redes, así como AWS que nos ofrece en el servicio VPC(Virtual private Cloud)1, que a grandes rasgos podemos crear un pequeño data center donde podemos realizar algo muy complejo, cabe resaltar que para entender la utilidad del servicio es necesario aprender el concepto de redes, es posible realizar una traducción del concepto de redes, el cual puede apreciarse en la Figura No. 1 e interpretarla como la Figura No. 2. La nube especialmente AWS de igual forma sabiendo incluso un poco de ITIL nos ofrecen servicios para poder aplicar dicha metodología, así bien como la gobernanza, el destacado catálogo de servicios2, incluso ofrece de manera intuitiva generar historial de cambios tales como lo hace el administrador de cambios, como se encuentra en el libro de transición. El conocimiento general que se adquiere en una educación formal ayuda mucho a comprender en gran medida servicios que nos ofrece la nube. Con el pasar de los años es más sencillo poder comprender el funcionamiento de los servicios y herramientas con tan solo adquirir un “pincelazo” de conocimiento. Conclusiones: Las bases en la educación que se reciben en centros especializados en tecnología tienen todo lo necesario, para ir y descubrir. La utilización de nuevas herramientas permite optimizar hasta un 50% del trabajo realizado, con tan solo tener conocimiento de los conceptos que se desean aplicar. Nunca hay perder la emoción de aprender algo nuevo, seguro en algún momento de la vida lo vamos a aplicar Referencias bibliográficas: AWS, documentación VPC (2019) AWS, documentación Service Catalogue Imagen 1 - Fuente: Elaboración propia. Imagen 2 - Fuente: Elaboración propia. "],
["10-cmejia.html", "Moderación de contenido, aplicación práctica de funciones Hash Conclusiones: Referencias bibliográficas:", " Moderación de contenido, aplicación práctica de funciones Hash Carlos Haroldo Mejía Díaz Knives50215@gmail.com Estudiante de Ingeniería en Ciencias y Sistemas - USAC Palabras Clave: Seguridad, Informática, Aplicación práctica, innovación Las plataformas sociales reciben demasiado contenido a diario, y no es de extrañarse que dentro de tanto contenido se encuentren algunos que no cumplan con los términos y condiciones de estas plataformas. Este contenido aparece de manera pública para que otros la puedan ver. Al tener volúmenes tan masivos, surge la pregunta ¿cómo se controla que el contenido sea legal? Facebook tiene aproximadamente 147,000 imágenes publicadas cada 60 segundos1. Para la moderación de las imágenes la solución tradicional ha sido la subcontratación. Imagen 1 - Fuente: PBS independant lens Existen servicios de monitoreo de imágenes, los cuales mantienen equipos de trabajo que se dedican a filtrar el contenido que ha sido subido a la plataforma en caso de que sea marcado como inapropiado por algún usuario. La mayoría de estos servicios están situados en diferentes partes del mundo, capacitando personal respecto a las políticas de las plataformas que monitorean para que sepan exactamente como clasificar el contenido como inapropiado o legítimo. “The Cleaners”2 es un documental realizado en 2018, el cual muestra el proceso que siguen estas instituciones de moderación de contenido. El documental nos muestra las consecuencias que conlleva la censura de contenido, tanto como para los usuarios como para los moderadores. El factor humano juega un papel importante en este proceso, la clasificación de imágenes con fines artísticos puede a veces confundirse con contenido explicito, sobrepasando los límites de los términos y condiciones de contenido publicable. El documental hace referencia a este tipo de casos y como es que se resuelven a través del entrenamiento y el criterio de los moderadores. Una de las desventajas que tiene esta solución es la carga psicológica que tienen esta tarea hacia las personas involucradas. Por Seguridad el personal encargado de la moderación accede a firmar un contrato de confidencialidad. No tienen permitido compartir o discutir con nadie el contenido de lo que han clasificado, incluyendo familiares, amigos o el utilizar servicios psicológicos. Estas personas revisan contenido que varía desde imágenes ofensivas hasta transmisiones en directo de suicidios o auto mutilación. Cada moderador tiene un sistema de creencias y este contenido impacta en su moralidad. Aunque el contenido sea para ellos moralmente inapropiado, si cumple con las regulaciones, este debe de ser marcado como seguro. En el documental se comparten diferentes perspectivas sobre el contenido que los moderadores manejan día a día. Algunos ven la posición como un deber en el cual ellos se sacrifican para que los demás no tengan que ver este contenido. Otros han aceptado que es un trabajo como todos los demás. Se comenta también de como la carga es tal que uno de los empleados decidió tomar su propia vida, ya que no pudo soportar el estrés y el impacto del trabajo. Afortunadamente podemos utilizar nuevas herramientas para reducir el factor humano en estas tareas, pudiendo evitar que más personas estén involucradas en la filtración de contenido. Los desarrollos en inteligencia artificial permiten hacer más fácil la identificación de objetos por medio de entrenamiento de redes neuronales, pudiendo aplicar estas técnicas para la clasificación de contenido ilegal o inapropiado. Dentro de estas herramientas existe PhotoDNA. Esta es una herramienta creada por Microsoft que ha donado a el centro nacional de niños desaparecidos y abuso de menores (National Center for Missing &amp; Exploited Children). Actualmente esta tecnología es usada por el buscador de Microsoft Bing, su sistema de almacenamiento OneDrive, Gmail, Twitter, Facebook y Adobe Systems. Microsoft dono esta tecnología hacia el proyecto VIC, el cual es soportado y administrado por el el Centro Nacional para Niños Perdidos y Explotados (NCMEC, por sus siglas en inglés). Esta tecnología usa las funciones hash para identificar de manera única imágenes de la base de datos que el FBI proporciona sobre pornografía infantil, y sobre estos hashes se hace la comparación con el contenido subido a las plataformas. De haber encontrado una coincidencia, se alerta a las autoridades correspondientes para que se tomen las medidas de seguridad establecidas. Pero, ¿cómo funciona esta tecnología? Primero debemos entender que es una función Hash. Esta son funciones las cuales toman un valor de entrada y lo operan mediante diferentes operaciones matemáticas para obtener un valor específico para la entrada. Estas funciones deben de seguir diferentes propiedades, por ejemplo: Estas deben de tomar una entrada y producir un valor el cual siempre será el mismo para el valor de entrada Debe de ser lo suficientemente compleja como para evitar llegar al valor de entrada por medio del valor obtenido También se debe de evitar colisiones, es decir, si un valor de entrada obtiene un valor con esta función, no debe de existir otra entrada que también obtenga el mismo valor Imagen 2 - Fuente: Microsoft, PhotoDNA Bajo estas características, se realiza un hash para todas las imágenes de la base de datos. Primero se convierte la imagen a escala de blanco y negro, para evitar que el uso de filtros afecte la detección de estas imágenes. Luego se cambia el tamaño de la imagen y se traza una rejilla para identificar sectores de diferente intensidad de gradientes y también de bordes característicos. PhotoDNA va un paso más adelante haciendo que los patrones que se identifiquen estén basados en biométrica, permitiendo la generación de códigos únicos para cada imagen respecto a la persona involucrada. Se aplica a esta imagen una función hash. Esta asegura el evitar las colisiones de 1 en diez billones. En el caso de que el material sean videos, el proceso se ejecuta para cada cuadro del video y se almacena un valor por cada uno. El tener estas imágenes para generar estos códigos hash es ilegal, por lo que PhotoDNA resolvió esta situación pidiéndole a las agencias de justicia que generen los códigos hash ellos mismos. Estos códigos son distribuidos para los que necesiten utilizarlos por motivos de filtrado de contenido. Debido al uso de biométrica para la generación de estos códigos, es posible el encontrar coincidencias no solo en las imágenes registradas, sino también en imágenes no registradas. Si se encuentra una coincidencia con alguna de las victimas ya identificadas se pude alertar a las autoridades sobre los posibles autores o distribuidores de este material. Este método ha sido exitoso para las empresas que lo utilizan. En 2013 esta tecnología ayudo a identificar a un usuario del servicio de almacenamiento de Verizon3, el cual cargo a su cuenta imágenes de pornografía infantil sin tener en cuenta que parte de los términos y condiciones de este servicio establecen que la plataforma escanea el contenido cargado para encontrar coincidencias de este tipo y alertar a las autoridades. Verizon no es la única plataforma con este tipo de tecnologías. Dropbox posee también una herramienta para identificar este tipo de anomalías, sin embargo, no ha sido posible identificar que herramienta estén utilizando. En el artículo “Dropbox Refuses to Explain Its Mysterious Child Porn Detection Software”4, Kate Knibbs trato de hablar con un representante de la compañía para saber más al respecto, sin conseguir una respuesta clara. Ella concluye que la razón es que estén tratando de evitar que los perpetradores utilicen algún tipo de fallo en este sistema para poder seguir cargando contenido ilegal. Anteriormente se ha establecido que Dropbox utiliza las funciones hash para revisar infracciones de contenido con copyright, utilizando el mismo procedimiento de generación de códigos de identificación únicos para el material registrado5. Este proceso no revisa los materiales personales lo cual lo hace perfecto para respetar la privacidad de los usuarios. El documental “The moderators”6 trata el mismo tema de monitoreo de contenido, mostrando el proceso de capacitación en el cual los moderadores se someten antes de clasificar contenido. El documental estima que existen aproximadamente más de 150,000 moderadores de contenido, lo cual es el doble del personal en google y casi 9 veces el personal en Facebook. “No puedes ser dependiente de la automatización” dice uno de los moderadores encargados de la capacitación de nuevo personal. “El sistema comete errores. Los humanos son necesarios, de eso estoy seguro.” “Podemos construir inteligencias artificiales, pero aun necesitas un operador” Explica otro moderador, “Definitivamente necesitas a una persona detrás de la maquina”. Con el desarrollo de tecnologías de detección de contenido ilegal, se espera reducir la carga de este tipo de equipos, restringiendo la exposición hacia ciudadanos ajenos a los departamentos de justicia. Conclusiones: La moderación de contenido es necesaria para las plataformas que almacenan contenido proporcionado por los usuarios. El desarrollo de nuevas tecnologías en inteligencia artificial puede utilizarse para la detección de contenido ilícito. Las funciones hash pueden utilizarse más allá de su propósito original de almacenamiento de datos hacia aplicaciones de identificación. Referencias bibliográficas: Facebook by the numbers. Disponible 10/4/2019 (1) The cleaners Disponible 10/4/2019 (2) How Verizon found child pornography in its cloud. Disponible 10/4/2019 (3) Dropbox Refuses to Explain Its Mysterious Child Porn Detection Software Disponible 10/4/2019 (4) How Dropbox Knows When You're Sharing Copyrighted Files. Disponible 10/4/2019 (5) The moderators. Disponible 10/4/2019 (6) Imagen 3 - Fuente: Screengrab The moderators, Field of vision "],
["11-lavila.html", "UI/UX - El diseño dentro del software Pensamientos erróneos Entendiendo el diseño La relación entre Ul/UX Aspectos importantes en el diseño Conclusiones: Referencias bibliográficas:", " UI/UX - El diseño dentro del software Leonel Eduardo Avila Calvillo leo.avilac8@gmail.com Estudiante de Ingeniería en Ciencias y Sistemas - USAC UI, UX, Usuario, Interfaz, Experiencia, Sistema, Software. Internet es el hogar de páginas sociales, páginas con recetas, aplicaciones de varios usos, etc. Existen un centenar de páginas web y aplicaciones cuyas funciones pueden llegar a ser idénticas entre sí, pero el modo en que cada una muestra la información o la manera en que el usuario interactúa con ellas es lo que las diferencia. Es por eso que la demanda por una interfaz de usuario efectiva es tan alta. Pero ¿qué significa UI/UX? y ¿cómo se relaciona con el desarrollo de software? UI son las siglas en ingles de User Interface, que significa Interfaz de Usuario, y UX son las siglas en ingles de User Experience, que significa Experiencia del Usuario. Estas dos definiciones representan dos prácticas al momento de desarrollar un software. La interfaz de usuario representa todo lo que el usuario ve al momento de iniciar el software, si la primera impresión que este recibe no es favorable, este suele elegir algún otro software con mejor apariencia. Mientras tanto, la experiencia del usuario es como el usuario percibe la funcionalidad del software, su manejabilidad dentro del sistema y como distintas opciones conllevan a otras partes dentro del sistema. Cuando estas dos prácticas son utilizadas juntas, el resultado obtenido es una mayor efectividad en el diseño del software. Pensamientos erróneos Uno de los problemas más comunes al momento de desarrollar un sistema es abocarnos directamente a escribir el código, sin tomar en consideración como el usuario va a utilizar el sistema, dejando la mayor parte de las veces la interfaz de usuario en el olvido, entrando a la línea de pensamiento de \"no se verá bien, pero funciona\". Si bien el funcionamiento del sistema es importante, no podemos dejar de lado el desarrollo de la interfaz gráfica. Debemos entender que cada parte del sistema, desde el backend hasta el frontend, debe estar bien analizado, sin embargo, debemos tener en cuenta que contar con una interfaz de usuario saturada de elementos, suele convertir la experiencia del usuario a algo desfavorable. Entendiendo el diseño Al momento de implementar la parte de diseño dentro de nuestro sistema, existen distintos factores que debemos tomar en cuenta. Dentro de estos factores tenemos el enfoque de \"el usuario primero\", pensando en cómo este utilizará el sistema, y como evitar que este se pierda dentro de la usabilidad del mismo. La relación entre Ul/UX Consideremos nuestro sistema como un proyecto de construcción. Un diseñador de UX suele ser el arquitecto, quien es el responsable de la estructura y de los planos del proyecto. Un diseñador de UI vendría siendo el diseñador de interiores de la casa, el que elige los colores, los muebles, el papel tapiz, entre otros. Estas dos prácticas se deben de trabajar juntas para desarrollar algo que se vea bien y que funcione igual de bien. Si la experiencia del usuario no congenia con el diseño del sistema, al usuario se le hará difícil navegar dentro de esta. Aspectos importantes en el diseño Al iniciar la elaboración de una nueva aplicación, muchas veces los desarrolladores se encuentran con distintas problemáticas. Una de las más destacadas es no saber por dónde empezar. Para dar una solución a esta problemática, podemos tomar en consideración distintos aspectos, tales como: Tener claro que queremos ofrecer con nuestro sistema, además de entender la problemática que vamos a solventar. El diseño debe estar centrado en el usuario. Pensar como actuarían ellos al navegar por las distintas opciones del programa, y como este puede confundirse con la usabilidad del sistema. Intenta encontrar que es lo más importante que quieres que haga el usuario. Una vez se haya identificado qué es lo principal, quita las distracciones u otros elementos que no tienen utilidad inmediatamente. Conclusiones: El desarrollo de software no se basa únicamente en programar la funcionalidad, también debemos pensar en cómo el usuario utilizará nuestro sistema, y como ofrecer la mejor experiencia. El ofrecer un mejor diseño y una mejor experiencia dentro de nuestro sistema, ayuda a que el usuario final elija nuestra aplicación sobre otras alternativas. Referencias bibliográficas: Luis Enrique Hernández Celaya. Diseño UI y UX. Claus Jepsen. Our expectations of software have changed - time UX cought up. "],
["12-egarcia.html", "Blockchains, un futuro cercano Conclusiones: Referencias bibliográficas:", " Blockchains, un futuro cercano Eder Yafeth Garcia Quiroa ey16_619@hotmail.com Estudiante de Ingeniería en Ciencias y Sistemas - USAC Palabras Clave: Blockchain, Bitcoins, Japón, Smart Contracts, Startup, Innovación, dApp, Mundo. Si bien ya se ha escrito mucho sobre Blockchain, ya que es un concepto que lleva más de 40 años de investigación, no es hasta el siglo XXI que se ha desvinculado totalmente del ámbito militar y se ha enfocado más en el cambio de paradigma con importantes consecuencias económicas. Son muchos los campos de aplicación para la Blockchain, iniciando con la criptografía que dio paso a una larga lista de criptomonedas teniendo a Bitcoin como moneda insignia, pasando por los llamados \"Mineros\" que tienen el rol de la creación de nuevos bloques y la verificación de los bloques añadidos a la Blockchain y dApp que no son más que app descentralizadas que están desarrolladas como un software que utiliza un token como activo de cambio para la gestión de sus transacciones, hasta llegar más allá del sector de las finanzas , como en el caso de la industria, salud, medios de comunicación e incluso los gobiernos. Pero ¿Cómo la Blockchain afectara nuestro futuro? Y la respuesta es... ¡ya lo está haciendo! en la actualidad Blockchain ha desarrollado nuevos modelos de negocios y mercados que van desde abrir y cerrar una puerta hasta el seguimiento de los pacientes que abandonen el hospital. Todo esto debido a que Blockchain se enfoca en acabar con los intermediarios, que hasta el día de hoy se necesitan para realizar operaciones y hacer de estos ecosistemas más rápidos y transparentes. Un claro ejemplo del poder de la Blockchain en la actualidad son los Smart Contracts, estos son capaces de ejecutarse y hacerse cumplir por sí mismo, de manera autónoma y automática, sin intermediarios ni mediadores. Evitan el lastre de la interpretación al no ser verbal o escrito en los lenguajes que hablamos. Los smart contracts se tratan de \"scripts\" escritos con lenguajes de programación, siendo los términos del contrato puras sentencias y comandos en el código que lo forma 2. Innovación en estado puro, es como se debería redefinir el concepto de Blockchain ya que ha tenido por consecuencia que una gran cantidad de startups trabajen en desarrollar plataformas o soluciones que utilizan Blockchain. Grandes empresas e incluso gobiernos están sumándose a estas iniciativas, tratando de ser pioneros en su aplicación sobre nuevos usos. Imagen 1 - Fuente: Desconocido Un caso reciente de Blockchain en un ámbito diferente al financiero, fue el de Virginia Occidental el primer estado en permitir a sus votantes usar una aplicación, basada en la tecnología Blockchain, este experimento comenzó en marzo y se completó con éxito el 8 de mayo del 2018. La aplicación está estaba destinada, sobre todo, a aquellos militares que están cumpliendo servicio en el extranjero 3. La aplicación fue desarrollada por el startup Voatz, con sede en Boston. Esta permitió a los votantes registrados en 24 países emitir boletas de voto a través de sus teléfonos celulares. Esta fue la solución más segura que consiguieron para facilitar el derecho al voto de los ciudadanos que actualmente hacen vida en el extranjero 3. La aplicación requirió de varios pasos de autenticación antes de que un votante reciba una boleta. Primero, a un usuario elegible se le envía un código de seis dígitos que debe ingresar para desbloquear la aplicación. Después de eso, Voatz requiere que los usuarios carguen una selfie y una identificación con una foto emitida por el gobierno. Las boletas se emiten después de que se verifican las dos imágenes. Una vez que se envían las boletas, los usuarios reciben un correo electrónico que confirma su escogencia. Finalmente, solo después de que se confirmen los votos, se los envía a las oficinas electorales locales en Virginia Occidental. Cabe destacar que, como medida de seguridad, la aplicación no se ejecutará si detecta algún malware en el teléfono 3. A esto hay que sumar que son varios los gobiernos que han estado experimentado con aplicaciones basadas en Blockchain, el caso de Venezuela y su criptomoneda Petro, los gobiernos de China, Rusia, Suiza, Reino Unido, Corea del Sur, Malta, Polonia, Japón, Estados Unidos son algunos de los que ya han activado su interés en las criptomonedas y sobre todo en la tecnología Blockchain. Además, ante el crecimiento de los mercados de Bitcoin en el mundo, también la tecnología Blockchain va ganando terreno, generando nuevas experiencias de organización política como Bitnation o Civic4. El continente asiático es el más activo en este sentido, y la apuesta de los gobiernos de la región por esta tecnología tiene varios importantes referentes como China, Corea del Sur, Singapur y Japón4. Este último, Japón, está realizando una gran apuesta de cara a los Juegos Olímpicos de Tokio 2020. Debido a que esta potencia mundial depende enormemente del dinero físico, y casi no practica los pagos electrónicos. Los japoneses tienen una debilidad, les gusta pagar en efectivo, la mayoría de los pagos realizados en el país se basa en billetes y monedas, perdiendo la batalla con sus vecinos asiáticos, China y Corea del Sur, donde el pago electrónico ha ido en aumento y dejándolo muy por detrás de Occidente, donde las tarjetas de crédito y débito son mucho más populares que él efectivo. Esto significa que el país necesita muchos cajeros automáticos, probablemente tiene más de 200.000, así como cajas registradoras y flotas de vehículos para mover el dinero. Todo esto supone un gasto anual de casi 16,000 millones de euros, la mayoría del cual corre a cargo del sector financiero5. En agosto del año pasado, el Gobierno anunció planes para ofrecer beneficios fiscales y subsidios a las empresas participantes. Y mientras que todo, desde los pagos con tarjeta de crédito hasta las transacciones con códigos QR, estaría incluido como pago sin efectivo, algunos de los expertos financieros más importantes del país piensan que la mejor forma de que Japón se deshaga del efectivo es mediante la tecnología Bitcoin5. Mitsubishi UFJ Financial Group (MUFG), el banco más grande del país y el quinto más grande del mundo a nivel de activos, se ha unido a la compañía estadounidense de internet Akamai para construir una red de pagos para el consumidor basada en Blockchain, a tiempo para los Juegos Olímpicos. Si lo logran, podrían generar la red de pagos de consumo más rápida y potente hasta la fecha5. En este proceso, Japón se convertirá en el proyecto piloto más grande del mundo para probar la Idea de que es posible utilizar un registro criptográfico y una red de ordenadores para crear una forma electrónica de dinero. La prueba incluso podría ayudar al país a recuperar su posición como líder mundial tanto en finanzas como en tecnología, un reinado que no ha tenido durante décadas5. Conclusiones: Blockchain se basa en la privacidad, esto sumado a una reducción en costo debido a la eliminación de intermediarios en el proceso y al no permitirse su alteración creó una tecnología disruptiva que van a cambiar muchas industrias. Esta tecnología aún son solo proyectos en pleno desarrollo, por lo que la revolución aún está lejos de dar todo su potencial, sobre todo cuando los intermediarios, en todos los ámbitos, se han convertido en parte integral de la economía y la sociedad. Referencias bibliográficas: Kunze, F. (2017). Introducción a Block.chain. (1) (07/04/2019) Academy, B. (2016). Smart contracts. ¿Qué son, cómo funcionan y qué aportan?. (2) (07/04/2019) Romero, R. (2018). Virginia Occidental usará aplicación basada en blockchain en las próximas elecciones. (3) (08/04/2019) Bastardo, J. (2017). Gobiernos del mundo optimizan sus procesos administrativos con tecnología blockchain 1 CriptoNoticias - Bitcoin, blockchains y criptoactivos. (4) (08/04/2019) Milutinovic, A. (2019). La apuesta de sustituir el efectivo por 'blockchain' se decide en Japón. (5) (08/04/2019) "],
["13-rlopez.html", "Big Data en el mundo del deporte ¿Quién fue su pionero dentro de este ámbito? Conclusiones: Referencias bibliográficas:", " Big Data en el mundo del deporte Rodney Estuardo López Marroquín rodx1497@gmail.com Estudiante de Ingeniería en Ciencias y Sistemas - USAC Palabras Clave: Big data, Tecnología, Deporte, Análisis, Datos. El análisis de los datos dentro del ámbito deportivo es cada vez más común en cualquier parte del mundo. Gracias a este tipo de observación dado en grandes volúmenes de datos, los cuales, se pueden tratar en diversos eventos deportivos, se facilita la oportunidad de hacer una mejora en la toma de decisiones dentro de este ámbito del deporte, tales como las tácticas que se puedan dar durante un partido, la gestión de fichajes, distancias recorridas, entre otros. Imagen 1 - Fuente: YouTube, User: Real Madrid Solo durante el desarrollo de algún partido se puede llegar a recolectar hasta 8 millones de datos, algo que a simple vista es casi imposible, ya que el ojo humano puede llegar a retener el 30% de lo sucedido durante el juego. Gracias a esto, el Big Data permite realizar conclusiones de manera sencilla con base a toda la información recibida previamente. ¿Quién fue su pionero dentro de este ámbito? El primer deporte en hacer uso del Big Data fue el béisbol, esto gracias a George William James, que, en aquel entonces era un aspirante a articulista deportivo de Kansas. Bill James, como también era conocido, empezó sus análisis a papel junto con su lápiz en los que tomaba en cuenta los registros históricos que tenían los jugadores de béisbol que militaban en las grandes ligas. Todo esto fue realizado a mediados de los años setenta, en donde los artículos de James eran de una forma totalmente distinta a lo que se conocía en esa época, él lo hacía con el fin de tomar en cuenta el juego que realizaba cada deportista en función de los datos estadísticos que se tuvieran disponibles, y no en criterios de una forma más subjetiva. Gracias a dicho trabajo realizado por James, se le considera a este como el padre de la Sabermetría, un análisis empírico general del béisbol. Mientras los primeros pasos del Big Data en el deporte se dieron de esta manera, en el año 2011 resurge en la ciudad de Hollywood gracias a la película Moneyball, dirigida por Bennet Miller y con Brad Pitt como protagonista, en la cual se recrea el trabajo de Billy Beane en los Oakland Athletics. Dicha película está basada en el libro publicado por Michael Lewis en el año 2003, donde se realiza un análisis de cómo el manager pudo formar un equipo competitivo, esto gracias al uso que les dio a las estadísticas obtenidas de cada uno de sus jugadores de béisbol. Es así como en deportes como el Hockey, específicamente en la NHL (Liga Nacional de Hockey estadounidense) la cantidad de personas que asisten a cada uno de sus encuentros ha aumentado en un 25%, todo gracias a que cada uno de ellos puede disfrutar de la gran cantidad de estadísticas que se puede llegar a tener a su alrededor, esto en tiempo real. También es el caso del ciclismo, gracias a Luca, un programa especial de Big Data de la empresa Telefónica, se obtienen diversos tipos de datos, esto a través de unos sensores que pueden llevar incorporados algunos ciclistas, tales como un potenciómetro o una banda de frecuencia cardíaca, lo que permite estudiar su comportamiento en carrera y enviar datos a la nube, para que, posteriormente, puedan ser subidos y se proceda a realizar su análisis de 3 fases. Dentro de estas 3 se ubica de primero la fase descriptiva, luego, le sigue la fase de predicción, y por último la fase de prescripción, lo que permite poder establecer entrenamientos específicos, para así, poder llegar en una buena forma a las fechas en que se disputará alguna actividad o torneo en especial. Asimismo, se puede visualizar cómo el análisis de los datos supone una revolución, no solamente en el mundo empresarial, también para el deporte profesional y otras áreas en la cual se puede desarrollar fácilmente. Imagen 2 - Fuente: YouTube, User: Real Madrid Conclusiones: El Big Data es una gran estrategia que permite el análisis de los datos en tiempo real, permitiendo obtener los resultados deseados en un lapso corto de tiempo. Actualmente, las diferentes entidades deportivas de algunos equipos están contratando a especialistas en Big Data, todo esto para poder encontrar nuevas ventajas competitivas. El análisis de los datos ha evolucionado conforme el tiempo, dejando atrás las técnicas que normalmente eran usadas, dando paso a una nueva era de procedimientos para el análisis y el manejo correcto, además de eficiente, de los datos. Referencias bibliográficas: Real Madrid C.F. (15/03/2019). Behind Real Madrid | Player performance. (22/03/2019). Ángel Carrillo. (28/11/2014). Bill James, el “padre” del Sabermetrics moderno. (25/03/2019). Mary Ann Liebert. (20/12/2018). How is big data impacting sports analytics? (26/03/2019). Ryan Ayers. How big data is revolutionizing sports. (27/03/2019). Imagen 3 - Fuente: YouTube, User: Real Madrid "],
["14-jsierra.html", "Desafíos y oportunidades del Cloud Computing en Guatemala ¿Por qué deberíamos de utilizar Cloud Computing? ¿De qué manera utilizamos Cloud Computing? ¿Qué oportunidades genera el Cloud Computing? ¿Qué desafíos nos genera utilizar Cloud Computing? Conclusiones: Referencias bibliográficas:", " Desafíos y oportunidades del Cloud Computing en Guatemala Julia Argentina Sierra Herrera julyargesh@gmail.con Estudiante de Ingeniería en Ciencias y Sistemas - USAC Palabras Clave: Paas, Saas, Iaas, Informacion, Seguridad, Costos Los servicios en la nube que se pueden encontrar son diversos, estos han realizado varios cambios en las empresas, la mayoría de estos positivos, permitiendo realizar tareas de manera sencilla. ¿Por qué deberíamos de utilizar Cloud Computing? El Cloud Computing, nos ofrece grandes oportunidades porque es lo que se está utilizando, además en la carrera de Ingeniería en Ciencias y Sistemas, la innovación es algo esencial porque el cambio es constante. En los últimos años América Latina ha hecho una importante apuesta por el desarrollo de las Tecnologías de la Información, situándose en una posición de liderazgo. De hecho, será la zona con el crecimiento más rápido del planeta, con un ritmo 1,74 veces superior que la media mundial, según publica Altag, que calcula que la tendencia de incremento de la industria TI será del 10,3% en 20131. En la imagen 1 se muestra como Guatemala no preparada para estos cambios, en esta misma grafica se muestran los demás países de Latinoamérica, a pesar que Guatemala no está situada en la última posición, podemos ver que ninguno de los países de Latinoamérica está preparado, lo que genera pérdidas, para empresas internacionales o diferentes trabajos donde se requiera el conocimiento de estas nuevas tecnologías, lo que nos permite visualizar que como estudiantes de ingeniería en sistemas es un compromiso, lograr que las gráficas cambien y nuestra preparación en estas nuevas tecnologías logren un avance no solo en una estadística si no que beneficien el desarrollo de nuestro país. Imagen 1 - Fuente: Cloud computing in Latin America Current situation and policy proposals, Economic Commission for Latin America and the Caribbean ECLAC. La revolución digital está trayendo consigo unas necesidades excepcionales de cómputo y almacenamiento. Muchas empresas tuvieron que crear sus propios centros de datos. Sin embargo, su coste es altísimo. La evolución definitiva vino de la mano del modelo Cloud Computing y la ideación y creación de un sistema completo (sostenible y de extremo a extremo) de prestación de servicios TI 2. ¿De qué manera utilizamos Cloud Computing? Las empresas comúnmente adquieren el Software como Servicio (SaaS, por sus siglas en inglés) porque carecen de recursos para instalar el software, y porque se paga por uso. El utilizar SaaS nos permite que los involucrados del proyecto, tengan acceso a la información desde cualquier dispositivo con el que puedan estar conectados a internet. Esto también proporciona una seguridad de datos, porque cuando se tengan fallos en algún dispositivo, la integridad de los datos no será dañada. Los servicios del Cloud Computing nos traen diferentes oportunidades, pero el servicio que más ha conseguido un impacto en las empresas ha sido laaS, porque al iniciar una empresa se reducen costos iniciales, permitiendo que sin poseer un espacio físico se tengan los recursos necesarios, obteniendo servicios por consumo, también permite cambios sobre lo que utilizamos, sin generar mayores gastos que se dan en sistemas físicos, lo que permite no tener que realizar cambios en información que generan pérdidas de tiempo y aumento de gastos. Plataforma como servicio, es utilizado por empresas orientadas a desarrollo de software, este servicio provee lo mismo que laaS con la diferencia que no se debe de preocupar por el mantenimiento. Las empresas eligen utilizar este servicio debido a que tienen herramientas de desarrollo y herramientas empresariales, que proporcionan reducción de tiempos en la programación. ¿Qué oportunidades genera el Cloud Computing? El Cloud Computing permite que los usuarios se beneficien reduciendo costos y tiempos, que son parte esencial en una empresa al momento de intentar lograr los conceptos de gerencia eficiencia y eficacia, se trata de utilizar la menor cantidad de recursos físicos, se ofrece optimizar recursos, cobros por tiempo de utilización, se evitan pérdidas de tiempo y se logra a la vez evitar contactos con distribuidores, técnicos, entre otros que generan gastos para la empresa. Logra que sea conveniente en la forma en que se adquieren servicios, la facilidad con que se realizan aplicaciones, e ingreso de datos, permite cambios de acuerdo a los requerimientos sin generar costos adicionales. Es fácil detectar en que se están realizando los gastos y permite visualizar de mejor manera como los cambios generados benefician o afectan las empresas. ¿Qué desafíos nos genera utilizar Cloud Computing? Al involucrarnos con la utilización del Cloud Computing, el idioma es uno de los desafíos debido a que la documentación y comunicación con los proveedores debe de ser en el idioma inglés. Los distribuidores de los servicios no se hacen cargo de las perdidas o robo de datos. Entonces el desafío como ingeniero en sistemas es identificar de qué manera se debería de realizar la estructura para utilizar Cloud Computing, conocer que datos están seguros dentro y fuera de los servicios que se están contratando. En la Imagen 2 se muestra que el 95% de los encuestados en Latinoamérica ven positivo el uso del Cloud Computing, además presenta que el 32% de los encuestados piensa que el inhibidor para su adopción es la seguridad. Por lo tanto, podemos determinar que la seguridad nos da un parámetro para decidir por que utilizar o no el Cloud Computing. Imagen 2 - Fuente: Comunidad BTZ Utilizar Cloud Computing genera nuevos desafíos para un programador, ingeniero en ciencias y sistema o cualquier otro cargo relacionado con tecnología, porque comprender de qué manera funcionan estos servicios genera el conocimiento de tecnologías que permiten que la utilización del Cloud Computing sea eficaz. Conclusiones: El desafío mas importante es mantenerse actualizado sobre la nueva tecnología que se está desarrollando. El Cloud Computing nos permite tener una empresa eficiente y eficaz. El desarrollo en el país también depende de una educación y compromiso con las nuevas tecnologías para poder ser desarrolladas. El Cloud Computing debe ser utilizado de la forma correcta para lograr que no se exponga la información importante. Referencias bibliográficas: Asper (16/12/2016). Los principales desafíos del cloud computing enviroment. (1) Comunidad Baratz (17/11/2013) La adopción e impacto del cloud computing en el mundo. (2) Evaluando Cloud (7/06/2015) Cloud Computing en America Latina (3) Evaluando Software (2017) Predicciones para la nube 2018. (4) Hernán Calderale. Desafios y oportunidades para los CIOs en la era de la nube. (5) "],
["15-rosorio.html", "Es hora de regular el contenido de internet Conclusiones: Referencias bibliográficas:", " Es hora de regular el contenido de internet Rubén Emilio Osorio Sotorro rubenosorio88@gmail.com Estudiante de Ingeniería en Ciencias y Sistemas - USAC Palabras Clave: Internet, tecnología, información, redes sociales, conocimiento, ciencia, regularización. Si bien el internet es una herramienta muy poderosa y necesaria en pleno siglo XXI, con millones y millones de páginas en las cuales podemos consultar el clima, que ofertas tendrá nuestro restaurante favorito, poder reservar un vuelo a la Patagonia simplemente con un par de clics en la comodidad de nuestro hogar, solo imaginemos un momento si en realidad todas las personas que tenemos acceso a internet lo usáramos para nutrir nuestros conocimientos, para hacer investigaciones y compartir resultados donde más personas se unan y se genere más conocimientos a partir de un trabajo colectivo como se observa en la imagen 1, pero claro, eso no significa que no se haga, la página del MIT publica muchos artículos que son base para muchas otras personas que pueden empezar una investigación o continuarla basándose en estos artículos, pero si hacemos una media de para que usamos más el internet en este siglo veremos que lo menos que se hace es investigar, muchos se enfocan más en el uso de redes sociales, intentando ser famosos de alguna forma, por ejemplo, al subir algo que muchos llamarían “gracioso” a páginas como Instagram, todo esto es el comienzo de una plaga llamada “influencers”. Como se observa en la imagen 1 un grupo de personas que únicamente se encargan de subir contenido (la mayoría basura en mi criterio) a Internet, y además hay marcas que por hacerles publicidad les pagan por ello al tener una gran cantidad de personas que las siguen y admiran, y ese en realidad es el problema, porque estas personas no son unos genios de la matemática, ni hablan sobre el cosmos, muchos se dedican a hacer retos verdaderamente tontos que no tienen algún sentido, como meterse un preservativo por la nariz y sacarlo por la boca como en la imagen 2, muchos repudian este tipo de contenido inútil, pero hay otros que si les llama la atención, y como ven que la persona famosa del internet lo hace, ellos también deben hacerlo. Imagen 1 - Fuente: Syda Productions Leyendo un artículo del famoso noticiero británico BBC que tiene como título “El 90% de lo que está en internet es basura”, en dicho escrito podemos encontrar que ellos describen un par de leyes de personas que se dedicaron a analizar las siguientes situaciones: Ley de Pommer, una persona puede cambiar de opinión en base a la información que lee en internet. La naturaleza de ese cambio será que pasará de no tener ninguna opinión a tener la opinión errada. Y es que esta ley tiene tanta razón con que hay personas creen todo lo que dice internet, y si hablamos de que la mayoría de contenido es basura, entonces la información también es basura. Imagen 2 - Fuente: Patricia Carambula Es donde comienzo a hablar sobre las restricciones que debe tener internet y es que si algo no está regulado se pierde el control, el problema a veces es ¿Quién lo regula? ¿Cómo decide que es bueno o malo?, pero es que es una realidad, la NASA podría hacer una transmisión desde marte que no muchas personas mirarían, pero uno de los videos más vistos en la plataforma de videos Youtube es un tipo cantando sobre una manzana, una piña, y un lápiz, visto por más de 255 millones de personas hasta este año 2019. Imagen 3 - Fuente: Youtube usuario: Pikotaro Está bien, páginas como Youtube no nos obligan a ver este tipo de contenido, el problema quizá es que este tipo de páginas alientan a las personas a seguir compartiendo este contenido ya que reciben un pago enorme por la cantidad de visitas que llegan a tener sus videos, pero sí utilizan algoritmos para hacer recomendaciones como “lo más visto” o los temas “hot” del momento, “El sistema de recomendación de vídeos de YouTube se enfrenta, una vez más, a la polémica. Algunos usuarios han criticado el algoritmo de la plataforma, alegando que promueve los vídeos de teorías de la conspiración y de fake news.”2 Y es que como todo en la vida el dinero es motivación, si no me van a pagar por hacer tonterías en internet, para que las voy a hacer, hace poco Mark Zuckerberg (creador de la famosa red social Facebook) hablaba de “Regular el internet” como se menciona en CNN “El fundador de Facebook, Mark Zuckerberg, publicó una misiva haciendo un pedido a los gobiernos y los reguladores para que impongan más controles sobre lo que se publica en internet, la protección de datos y otro temas”.3 Zuckergerg propone 4 aspectos que requieren especial cuidado: Sobre el contenido perjudicial: Mark menciona que se debe estandarizar y hacer más rígidas las políticas sobre el contenido que se sube a cada plataforma y en sus procesos, se necesita un enfoque más estandarizado. Integridad en las elecciones: También menciona que deben haber leyes que se aplican en cada gobierno que debe regular que clase de contenido es permitido por cada candidato a hacerse publicidad, también de esto viene el tema de los “netcenters” y encuestas falsas donde posicionan a un candidato a favor sobre los otros. Privacidad de los datos: si bien todo lo que está e internet pareciera no ser privado para nada, “La nueva regulación de privacidad en los Estados Unidos y en todo el mundo debería basarse en las protecciones que proporciona el Reglamento General de Protección de Datos (GDPR, por sus siglas en inglés). Éste debería proteger tu derecho a elegir cómo se usa tu información y no de requerir que los datos se almacenen de manera local, ya que lo haría más vulnerable al acceso injustificado.”4 Portabilidad en los datos: aquí se refiere que hay que aclarar las reglas sobre quien es responsable de proteger la información que las personas cuando esta se mueve entre cada servicio. En mi punto de vista personal, contenido que no promueve nada bueno en las personas es perjudicial, perjudicial para sus mentes. Conclusiones: Se debe dar menos prioridad a videos y contenidos de influencers en las redes pagándoles poco sobre su contenido. La regulación es un tema que se da abasto y se deben tener muchos debates al respecto. El contenido que cada persona consume en internet es siempre su responsabilidad, pero las plataformas deben de promover contenido más informativo, ya depende del usuario si lo ve o no. Referencias bibliográficas: BBC Mundo (26/01/2014) El 90% de lo que está en internet es basura. Xose Llosa (12/2/2018). Youtube dejará de visibilizar a creadores tóxicos Xavier Serbia (01/04/2019). ¿Qué sugiere Facebook para regular internet? Sandra Pérez (02/04/2019). 4 Formas de regular internet según Zuckerberg. "]
]
